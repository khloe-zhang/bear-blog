---
title: "ç½‘ç«™æ­å»ºæŠ€æœ¯æ€»ç»“ï¼ˆäºŒï¼‰åŠ¨æ€ç¯‡"
author: ["Bear"]
date: 2025-10-07T16:30:00+08:00
keywords: 
- web
categories: 
- tech
tags: 
- web
- AWS
- EC2
- Nginx
- RAG
- AI
description: "æ­å»ºåŠ¨æ€ç½‘é¡µï¼ˆAPIè°ƒç”¨ï¼‰ç”¨åˆ°çš„ Web ç›¸å…³æŠ€æœ¯æ ˆ"
weight:
slug: ""
draft: false # æ˜¯å¦ä¸ºè‰ç¨¿
comments: true
reward: false # æ‰“èµ
mermaid: false # æ˜¯å¦å¼€å¯mermaid
showToc: true # æ˜¾ç¤ºç›®å½•
TocOpen: true # è‡ªåŠ¨å±•å¼€ç›®å½•
hidemeta: false # æ˜¯å¦éšè—æ–‡ç« çš„å…ƒä¿¡æ¯ï¼Œå¦‚å‘å¸ƒæ—¥æœŸã€ä½œè€…ç­‰
disableShare: true # åº•éƒ¨ä¸æ˜¾ç¤ºåˆ†äº«æ 
showbreadcrumbs: true # é¡¶éƒ¨æ˜¾ç¤ºè·¯å¾„
cover:
    image: "posts/tech/web-dynamic/chatpage.png"
    caption: "" # å›¾ç‰‡åº•éƒ¨æè¿°
    alt: ""
    relative: false
---

ä¹¦æ¥ä¸Šå›[ç½‘ç«™æ­å»ºæŠ€æœ¯æ€»ç»“ï¼ˆä¸€ï¼‰é™æ€ç¯‡](https://bearlybear.com/posts/tech/web-static/)ï¼Œå…¶å®åœ¨åˆšå¼€å§‹æ¥è§¦ AI çš„æ—¶å€™ï¼Œæˆ‘å°±æœ‰æƒ³è¿‡åšä¸€ä¸ªå…³äºæˆ‘è‡ªå·±çš„ AI é—®ç­”æœºå™¨äººï¼Œåªæ˜¯å½“æ—¶è¿˜ä¸æ¸…æ¥šå¦‚ä½•å®ç°ã€‚åœ¨äº†è§£åˆ° RAG ä¹‹åï¼Œæœ‰äº†çµæ„Ÿâ€”â€”æŠŠæˆ‘çš„ä¿¡æ¯éƒ½æŒ‚è½½åˆ° RAG çŸ¥è¯†åº“é‡Œï¼Œè®© AI å»æ£€ç´¢ï¼Œå°±å¯ä»¥ç”Ÿæˆå…³äºæˆ‘çš„å›ç­”äº†ï¼åšå®¢ç½‘ç«™å·²ç»æœ‰äº†é›å½¢ï¼Œæ˜¯æ—¶å€™æŠŠè¿™ä¸ªåŠŸèƒ½åŠ ä¸Šäº†ã€‚

# æ¶æ„è®¾è®¡

æˆ‘é‡‡ç”¨äº†åŠ¨é™åˆ†ç¦»çš„æ¶æ„ï¼ŒåŸå…ˆé…ç½®å¥½çš„åšå®¢æ¶æ„ã€CSS æ ·å¼ã€æ–‡ç« ç­‰é™æ€èµ„æºä¸éœ€è¦è¿ç§»èµ°ï¼Œè¿˜å¯ä»¥æ”¾åœ¨åŸæ¥çš„ S3 é‡Œï¼Œåªéœ€è¦æŠŠ API æ¥å£è°ƒç”¨çš„é€»è¾‘å•ç‹¬è·‘åœ¨æœåŠ¡å™¨é‡Œå³å¯ï¼Œåªæœ‰ç”¨æˆ·å‘é€ API è¯·æ±‚çš„æ—¶å€™ï¼Œæ‰ä¼šå»è°ƒç”¨åŠ¨æ€èµ„æºè¿”å›ç»“æœã€‚åç«¯æˆ‘é‡‡ç”¨äº† FastAPIï¼Œå› ä¸ºå¹´åˆä½¿ç”¨ Flask çš„ä½“éªŒä¸æ˜¯å¾ˆå¥½ï¼›ä¸ºäº†å’Œå·²æœ‰çš„åŸºç¡€è®¾æ–½ä¸æ»‘å¯¹æ¥ï¼Œæˆ‘è¿˜æ˜¯ç»§ç»­ç”¨ AWSï¼Œé€‰æ‹©äº† EC2 ä½œä¸ºè™šæ‹Ÿ Linux ä¸»æœºã€‚

å…³äº AI çš„éƒ¨åˆ†ï¼Œå°½ç®¡ç°åœ¨æœ‰å¾ˆå¤šä¸€ç«™å¼çš„ AI åº”ç”¨å¼€å‘å¹³å°ï¼Œå¯ä»¥ç›´æ¥å®ç° RAG çš„ä¸€å¥—æµç¨‹ï¼ˆè¯åµŒå…¥ + å‘é‡æ£€ç´¢ï¼‰ä»¥åŠ API å°è£…è°ƒç”¨ï¼Œä½†æˆ‘è¿˜æ˜¯é€‰æ‹©äº†æœ¬åœ°å®ç° RAGï¼Œä½¿ç”¨ Transformer åŠ è½½è¯åµŒå…¥æ¨¡å‹ï¼Œé…åˆ ChromaDB å‘é‡æ•°æ®åº“ï¼Œæœ€åè°ƒç”¨ Deepseek API ç”Ÿæˆå›ç­”ã€‚æˆ‘çš„æƒ³æ³•æ˜¯åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å°½å¯èƒ½å¤šåœ°æ¥è§¦ RAG çš„åŸå§‹éƒ¨ç½²æµç¨‹ï¼Œæ›´ç›´è§‚åœ°æ‹†è§£å’Œå­¦ä¹  RAG çš„åŸç†ã€‚

ä¸ºäº†æ›´å¥½åœ°ç›‘æ§æœåŠ¡å™¨æ€§èƒ½å’Œ API æŒ‡æ ‡ï¼Œæˆ‘é…ç½®äº† Prometheus + Grafana ä¸¤ä»¶å¥—ã€‚æˆ‘ç”¨ node-exporter é…åˆ Prometheus æŠ“å–æœåŠ¡å™¨è¿è¡ŒçŠ¶å†µï¼Œå±•ç¤ºåœ¨ Granafa ä¸­ã€‚æ­¤å¤–æˆ‘è¿˜åœ¨ API æœåŠ¡çš„åç«¯é…ç½®äº† API è°ƒç”¨çš„ä¸€äº›æŒ‡æ ‡æ–­ç‚¹ï¼Œæ¯”å¦‚æ€»æé—®é¢‘ç‡ã€æé—®è®°å½•ä¸å›ç­”è®°å½•ã€å…¨ç¨‹è€—æ—¶ï¼Œä»¥åŠç”¨äº RAG ä¼˜åŒ–çš„ä¸€äº›æµ‹è¯•æŒ‡æ ‡ï¼Œé€šè¿‡ SQLite è®°å½•ä¸‹æ¥å¹¶ä¸”ä¼ åˆ° Grafana çš„å¯è§†åŒ–é¢æ¿ä¸­ï¼Œä¾¿äºåç»­ä¼˜åŒ– API æœåŠ¡çš„è¡¨ç°ã€‚

# AWS EC2

## åˆ›å»ºå®ä¾‹
æœ¬ç€æˆæœ¬æœ€å°åŒ–çš„ç†å¿µï¼Œå½“ç„¶è¿˜æ˜¯è¦è¹­ AWS çš„å…è´¹å¥—é¤ã€‚æˆ‘é¢„ä¼° API è¯·æ±‚çš„æ¬¡æ•°ä¸ä¼šå¾ˆå¤šï¼Œå‡ ååˆ°å‡ ç™¾è¯·æ±‚/å¤©ï¼Œè€Œä¸”åç«¯æœåŠ¡æµç¨‹ä¸å¤æ‚ï¼Œt2.micro åº”è¯¥å¤Ÿç”¨ã€‚Linux é€‰æ‹© ubuntu 24.04ï¼ŒEBS å­˜å‚¨å®¹é‡é»˜è®¤ 8Gï¼ˆåæ¥è¯æ˜å¤ªå°äº†ï¼Œæœ€å¥½æ”¹æˆ 10Gï¼‰ã€‚

åˆ›å»ºå®ä¾‹åï¼Œè®°å¾—ä¿å­˜å¯†é’¥æ–‡ä»¶çš„ä½ç½®ï¼Œä»¥åç”¨ ssh ç™»å½•çš„æ—¶å€™éœ€è¦åˆ‡æ¢åˆ°è¿™ä¸ªç›®å½•ä¸‹ã€‚ä¸ºäº†åç»­ç½‘ç»œé…ç½®æ–¹ä¾¿ï¼Œæˆ‘ç”³è¯·äº†ä¸€ä¸ªå¼¹æ€§ IPï¼Œè¿™æ · EC2 å°±æœ‰äº†ä¸€ä¸ªå›ºå®šçš„å…¬ç½‘ IP åœ°å€ï¼Œè€Œä¸ä¼šæ¯æ¬¡é‡å¯ IP éƒ½è¦å˜ã€‚

æ¥ä¸‹æ¥é…ç½®å®‰å…¨ç»„ï¼Œåœ¨åˆ›å»ºå®ä¾‹çš„è¿‡ç¨‹ä¸­å…ˆå®šä¹‰ HTTPã€HTTPSã€SSH è¿™ä¸‰ä¸ªç±»å‹çš„å‡ºå…¥ç«™è§„åˆ™ï¼Œåç»­æ ¹æ®æœåŠ¡çš„å¢åŠ è¿˜ä¼šéœ€è¦å›æ¥è°ƒæ•´çš„ã€‚å‡ºç«™æµé‡æ˜¯é»˜è®¤å…è®¸æ‰€æœ‰ï¼›å…¥ç«™æµé‡ä¸­ï¼ŒHTTP å’Œ HTTPS å¯å…è®¸æ‰€æœ‰ï¼ˆ0.0.0.0ï¼‰ï¼Œä½† SSH ä»…å…è®¸æœ¬æœº IP åœ°å€ã€‚ä»¥åç”µè„‘æ¯æ¬¡å¼€æœºæˆ–è€…æ¢äº†ç½‘ç»œè¿æ¥æ—¶ï¼Œè¦è®°å¾—å›æ¥æ”¹ä¸€ä¸‹è¿™ä¸ªå®‰å…¨ç»„è§„åˆ™ã€‚åç»­ç”±äºæˆ‘å¢åŠ äº†æœåŠ¡ï¼Œéœ€è¦æ–°å¢â€œè‡ªå®šä¹‰ TCP ç±»å‹â€çš„ç«¯å£æ”¾è¡Œè§„åˆ™ï¼šAPI çš„æœåŠ¡ç«¯å£æ”¾åœ¨äº† 8000ï¼ŒPrometheus ç«¯å£ 9090ï¼ŒGrafana ç«¯å£ 3000ï¼Œè¿™äº›ç«¯å£ä¹Ÿè®¾ç½®äº†ä»…å…è®¸æœ¬æœºåœ°å€è®¿é—®ã€‚è¿™ä¹‹åå°±å¯ä»¥é€šè¿‡ SSH è¿æ¥å®ä¾‹äº†ã€‚

åœ¨è¿™é‡Œè¿˜è¦è¯´ä¸€ä¸‹ï¼Œæœ€å¼€å§‹é…ç½®å®Œå®ä¾‹åï¼Œé™¤äº† SSH è¿™ä¸ªè¿æ¥æ–¹å¼ä»¥å¤–ï¼Œæœ€å¥½ç•™ä¸€ä¸ªåé—¨ï¼Œä»¥é˜² SSH ç”±äºå„ç§çŠ¶å†µæ— æ³•è¿æ¥ã€‚æˆ‘å°±é‡åˆ°è¿‡ systemd æœåŠ¡å¾ªç¯é‡å¯å¡æ­»å®ä¾‹ï¼ŒSSH å‘½ä»¤è¡Œçª—å£å¡æ­»ï¼Œé€€å‡ºå†ç™»å°±æ­»æ´»ç™»ä¸ä¸Šäº†ã€‚ä¸€å¼€å§‹æˆ‘è¿˜æ–°å»ºä¸€ä¸ªæ•‘æ´å®ä¾‹ï¼Œç„¶åæŠŠ EBS å·åˆ†ç¦»æŒ‚åˆ°è¿™ä¸ªæ–°å®ä¾‹ä¸Šï¼Œç™»å½•æ–°å®ä¾‹æŠŠåº”ç”¨æœåŠ¡æ•´ä¸ªåˆ æ‰å†æŠŠå·æŒ‚å›å»ï¼Œéå¸¸éº»çƒ¦ã€‚è§£å†³æ–¹æ³•æ˜¯ï¼Œå¯ä»¥é…ç½®ä¸€ä¸ªâ€œä¼šè¯ç®¡ç†å™¨â€çš„ç™»å½•æ¸ é“ï¼Œè¿™æ˜¯ AWS æä¾›çš„ä¸€ç§åŸºäº System Manager çš„è¿æ¥æ–¹å¼ï¼Œæ— éœ€ç»è¿‡ SSHï¼Œç›´æ¥åœ¨æµè§ˆå™¨ç«¯å°±å¯ä»¥ä¸€é”®è¿æ¥å®ä¾‹ï¼Œå¹¶ä¸”æœ‰ç½‘é¡µç«¯çš„ CLIã€‚å…·ä½“çš„é…ç½®æ–¹å¼å¯ä»¥å‚è€ƒ [ä½¿ç”¨ä¼šè¯ç®¡ç†å™¨è¿æ¥åˆ° Amazon EC2 å®ä¾‹](https://docs.aws.amazon.com/zh_cn/prescriptive-guidance/latest/patterns/connect-to-an-amazon-ec2-instance-by-using-session-manager.html)ã€‚å¤§æ¦‚æµç¨‹å°±æ˜¯å…ˆå» IAM é‡Œï¼Œé€‰æ‹©â€œåˆ›å»ºè§’è‰²â€ï¼Œæ ¹æ®æŒ‡å¼•é€‰æ‹©å¯¹è±¡ä¸º EC2ï¼Œç„¶ååœ¨â€œä½¿ç”¨æ¡ˆä¾‹â€é‡Œé€‰æ‹© EC2 Role for AWS Systems Managerï¼Œæ¥ç€ä¸€è·¯é»˜è®¤æ“ä½œå°±å¯ä»¥äº†ã€‚é…ç½®å¥½åï¼ŒEC2 å®ä¾‹è¿æ¥é¡µé¢çš„ä¼šè¯ç®¡ç†å™¨çš„â€œè¿æ¥â€é”®å°±ä»ç°è‰²å˜æˆå¯æ“ä½œäº†ã€‚åå¤„å°±æ˜¯è¿™ä¸ª CLI å¾ˆå¤šå¿«æ·é”®éƒ½ç”¨ä¸äº†ï¼Œè¿å¤åˆ¶ç²˜è´´éƒ½è¦é é¼ æ ‡å³é”®ï¼Œç”¨ nano ç¼–è¾‘è„šæœ¬æ–‡ä»¶ä¹Ÿå¾ˆéº»çƒ¦ã€‚å¹¶ä¸”ï¼Œæ¯æ¬¡è¿›å»éœ€è¦æ‰‹åŠ¨åˆ‡æ¢åˆ° ubuntu ç”¨æˆ·ï¼š`sudo -i -u ubuntu`ã€‚

## å®‰è£…å„ç§è½¯ä»¶åŒ…
é€šè¿‡ SSH ç™»å½• Linux ä¸»æœºåï¼Œå…ˆå®‰è£…ä¸€äº›åŸºç¡€çš„è½¯ä»¶åŒ…ï¼Œæ¯”å¦‚ python3-pipã€gitã€nginxã€‚

```shell
# 1. æ›´æ–°è½¯ä»¶åŒ…åˆ—è¡¨ï¼ˆå¯¹äºUbuntuï¼Œä½¿ç”¨aptï¼‰
sudo apt update && sudo apt upgrade -y

# 2. å®‰è£…å¿…è¦çš„è½¯ä»¶åŒ…
sudo apt install -y python3-pip python3-venv git nginx
```

ç„¶ååœ¨ ubuntu ç”¨æˆ·çš„ home ç›®å½•ä¸‹å»ºä¸€ä¸ªé¡¹ç›®ç›®å½•ï¼Œå­˜æ”¾é¡¹ç›®çš„æ‰€æœ‰ä»£ç ã€‚ä¸ºäº†é˜²æ­¢é¡¹ç›®çš„åº“ä»¥åå’Œåˆ«çš„é¡¹ç›®äº§ç”Ÿä¾èµ–å†²çªï¼Œå¯ä»¥ç»™è¿™ä¸ªé¡¹ç›®å•ç‹¬å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œå¦‚æœåœ¨é…ç½®å‰æœŸå‡ºäº†ä»€ä¹ˆé—®é¢˜ï¼Œå°±å¯ä»¥ç›´æ¥åˆ æ‰è¿™ä¸ªç¯å¢ƒé‡æ¥ã€‚

```shell
python3 -m venv venv  # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
```

ç„¶åï¼Œå¼€å§‹å®‰è£…è½¯ä»¶çš„æ¼«æ¼«é•¿è·¯å§ï¼

å…ˆä»ç®€å•çš„å¼€å§‹ï¼Œåˆ›å»ºä¸€ä¸ª requirement.txtï¼ŒæŠŠè¦è£…çš„è½¯ä»¶éƒ½å†™è¿›å»ã€‚æœ€å¼€å§‹æˆ‘å†™çš„æ˜¯ä¸‹é¢è¿™æ ·çš„ï¼šï¼ˆä¸è¦å¤åˆ¶è¿™ä¸ªæ¥ç”¨ï¼ä»…åšè®°å½•ã€‚åé¢äº§ç”Ÿäº†è¶…å¤šåŒ…å†²çªå’Œå†…å­˜ä¸è¶³çš„é—®é¢˜ï¼‰

```shell
cat > requirements.txt << EOL
fastapi[standard]
uvicorn[standard]
chromadb
sentence-transformers
openai
prometheus-client
EOL

pip install -r requirements.txt
```

è¿™é‡Œå…ˆè¯´ä¸€ä¸‹å„ä¸ªåŒ…çš„ä½œç”¨ã€‚fastapi ç”¨æ¥æ„å»ºåŸºäº Python è¯­è¨€çš„åç«¯ï¼›uvicorn æ˜¯ç”¨æ¥åœ¨åç«¯ä¸­åšå¼‚æ­¥è¿›ç¨‹ç®¡ç†çš„ï¼›chromadb æ˜¯å‘é‡æ•°æ®åº“â€”â€”åœ¨ RAG æ—¶ï¼Œéœ€è¦å…ˆç”±ä¸€ä¸ªè¯åµŒå…¥æ¨¡å‹å°†æé—®å’ŒçŸ¥è¯†åº“éƒ½è½¬ä¸ºå‘é‡è¡¨ç¤ºï¼Œç„¶åå»å‘é‡æ•°æ®åº“é‡Œæ£€ç´¢ç›¸å…³çŸ¥è¯†ã€‚sentence-transformers çš„ä½œç”¨æ˜¯ï¼š
> Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art embedding and reranker models.

æ¥æºï¼š[SBERT å®˜ç½‘](https://www.sbert.net/)ã€‚å°±æ˜¯ç”¨æ¥åŠ è½½è¯åµŒå…¥å’Œæ’åºæ¨¡å‹çš„ä¸€ä¸ªå·¥å…·æ¨¡å—ã€‚


å‡†å¤‡å¥½å®‰è£…åˆ—è¡¨ï¼Œç„¶åå°±å¼€å§‹ä¸åœåœ°å¤„ç†æŠ¥é”™äº†......

é¦–å…ˆç™»åœºçš„æ˜¯ï¼ŒChromaDB çš„ä¾èµ–åº“é‡Œï¼Œurllib3 è¿™ä¸ªåº“æœ‰ nvidia_cudnn_cu12 è¿™ä¸ªåŒ…ï¼Œå®‰è£…çš„æ—¶å€™æˆ‘è¿™ä¸ç‰ˆå®ä¾‹çš„ç£ç›˜ç©ºé—´ä¸è¶³æŠ¥é”™äº†ã€‚ç”±äºæˆ‘ä¸éœ€è¦å®‰è£…å®Œæ•´çš„CUDAå·¥å…·åŒ…ï¼ˆè¿™æ˜¯ä¸ºGPUæœºå™¨å­¦ä¹ å‡†å¤‡çš„ï¼‰ï¼Œå¯¹äºCPUç¯å¢ƒï¼Œå¯ä»¥é€‰æ‹©æ›´è½»é‡çš„ä¾èµ–ã€‚ç°åœ¨ä¹Ÿä¸çŸ¥é“åˆšæ‰é‚£äº›åŒ…å®‰è£…åˆ°ä»€ä¹ˆç¨‹åº¦äº†ï¼Œæ‰€ä»¥ç›´æ¥åˆ æ‰è™šæ‹Ÿç¯å¢ƒé‡æ¥ã€‚

```shell
# é€€å‡ºå½“å‰è™šæ‹Ÿç¯å¢ƒ
deactivate

# åˆ é™¤å½“å‰å¤±è´¥çš„è™šæ‹Ÿç¯å¢ƒ
rm -rf /home/ubuntu/<é¡¹ç›®æ–‡ä»¶å¤¹>/venv

# æ¸…ç†pipç¼“å­˜é‡Šæ”¾ä¸€äº›ç©ºé—´
pip cache purge
```

è·Ÿåˆšæ‰ä¸€æ ·ï¼Œé‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œç„¶åæ–°å»ºä¸€ä¸ª requirement.txtï¼ŒæŒ‡å®šç‰ˆæœ¬ï¼Œå•ç‹¬å®‰è£… ChromaDB ä¾èµ–ï¼Œé¿å¼€ CUDA åŒ…ï¼Œå¹¶ä¸”æŒ‡å®šå„ç§è½¯ä»¶çš„ç‰ˆæœ¬ã€‚

```shell
cat > requirements.txt << EOL
fastapi==0.104.1
uvicorn[standard]==0.24.0
sentence-transformers==2.2.2
openai==1.3.0
prometheus-client==0.19.0
# ä½¿ç”¨chromadbçš„è½»é‡ç‰ˆï¼Œé¿å…CUDAä¾èµ–
chromadb==0.4.15 --no-deps
# å•ç‹¬å®‰è£…chromadbçš„CPUä¾èµ–
pypika==0.48.9
posthog==2.4.2
typing-extensions==4.8.0
urllib3==1.26.16
# æ·»åŠ å…¶ä»–å¿…è¦ä¾èµ–
numpy==1.24.3
pydantic==2.5.0
torch==2.5.1+cpu
EOL
```

æ¥ä¸‹æ¥ï¼Œä¸ºäº†é˜²æ­¢å®‰è£…æ—¶ä¸­é—´é‡åˆ°æŠ¥é”™åœä¸‹æ¥ï¼Œå†å‡ºç°ä¸çŸ¥é“å®‰è£…åˆ°äº†å“ªä¸€æ­¥ç„¶åå…¨éƒ¨åˆ æ‰çš„æƒ…å†µï¼Œè¿™æ¬¡å°±ä¸ç”¨ pip install -r requirements.txt ä¸€æ¬¡å®‰è£…ï¼Œè€Œæ˜¯åˆ†æ­¥å®‰è£…ã€‚

```shell
# å…ˆå®‰è£…æœ€åŸºç¡€çš„åŒ…
pip install fastapi uvicorn prometheus-client openai

# å®‰è£…sentence-transformersï¼ˆæŒ‡å®šä¸ä½¿ç”¨CUDAï¼‰
pip install sentence-transformers --no-deps
pip install transformers tokenizers torch --index-url https://download.pytorch.org/whl/cpu

# å®‰è£…chromadbåŠå…¶æœ€å°ä¾èµ–
pip install chromadb --no-deps
pip install pypika posthog typing-extensions urllib3

# å®‰è£…å…¶ä»–å¿…è¦ä¾èµ–
pip install numpy pydantic
```

è¿™ä¸­é—´åˆé‡åˆ°äº†å¾ˆå¤šæ¬¡æŠ¥é”™ï¼Œä¸»è¦æ˜¯åº“ä¹‹é—´çš„ç‰ˆæœ¬å†²çªé—®é¢˜ã€‚è¿™é‡Œæˆ‘æ€»ç»“ä¸€ä¸‹æœ€åèƒ½æˆåŠŸå…¼å®¹çš„å‡ ä¸ªåº“çš„ç‰ˆæœ¬ï¼š

- Python 3.10.18
- Torch 2.5.1+cpu
- Transformers 4.56.1

å®‰è£…å®Œæˆåï¼Œå¯ç”¨äºæµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸçš„å‘½ä»¤ï¼Œå°±æ˜¯importä¸€ä¸‹ï¼Œå¦‚æœæ²¡æœ‰æŠ¥é”™å°±æ˜¯å®‰è£…æˆåŠŸäº†ï¼Œä¹Ÿå¯ä»¥æ‰“å°å‡ºç‰ˆæœ¬å·ï¼š

```shell
python -c "
import fastapi
import uvicorn
from sentence_transformers import SentenceTransformer
print('æ ¸å¿ƒåŒ…å¯¼å…¥æˆåŠŸï¼')
"

python -c "import transformers
print(transformers.__version__)"

# ä¹Ÿå¯ä»¥é€šè¿‡ pip å‘½ä»¤åˆ—å‡ºæŒ‡å®šåŒ…ä»¥åŠç‰ˆæœ¬
pip list | grep -E "(torch|transformers|sentence)"
```

é‡ç£…æ’æ›²ï¼šå®‰è£…è¿‡ç¨‹ä¸­ï¼Œsentence_transformer å’Œ huggingface_hub åº“æœ‰å†²çªï¼ŒåŸå› æ˜¯ 0.26.0 ç‰ˆæœ¬ä»¥ä¸Šçš„ huggingface_hub å·²ç»ç§»é™¤äº† cached_download è¿™ä¸ªå‡½æ•°ï¼Œä½†æ˜¯ sentence_transformer ä»ç„¶åœ¨å®‰è£…è„šæœ¬ä¸­ import å®ƒï¼ŒæŠ¥é”™å¦‚ä¸‹ï¼š

`
File "/home/ubuntu/<é¡¹ç›®æ–‡ä»¶å¤¹>/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py", line 12, in <module>
from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_url, cached_download
ImportError: cannot import name 'cached_download' from 'huggingface_hub'
`

è¿™ä¸ªé—®é¢˜èŠ±äº†å¥½å¤§ä¸€ç•ªåŠŸå¤«åˆ°å¤„æƒ³åŠæ³•ã€‚æˆ‘ä¸€å¼€å§‹å°è¯•è¿‡é™çº§ huggingface åº“åˆ° 0.25.0ï¼Œä¹Ÿå°±æ˜¯è¿˜ä¿ç•™ cached_download çš„æœ€åä¸€ä¸ªæ—§ç‰ˆæœ¬ï¼Œä½†æ˜¯æŠ¥é”™ï¼š

`
transformers 4.56.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 0.25.0 which is incompatible. `

å¥½å®¶ä¼™ï¼Œé™·å…¥å¥—å¨ƒå¾ªç¯äº†ï¼Œç‰ˆæœ¬é«˜äº†å°±æœ‰å‚æ•°è¿‡æ—¶çš„é—®é¢˜ï¼Œç‰ˆæœ¬ä½äº† Transformers åˆä¸æ”¯æŒã€‚ä¹Ÿä¸æ•¢å†å»é™çº§ Transofmers åº“äº†ï¼Œä¹‹å‰å¤„ç† Torch å†²çªå°±èŠ±äº†åŠå¤©ï¼Œå¥½ä¸å®¹æ˜“æ‰å¼„å¥½ï¼Œè°çŸ¥é“è¿˜ä¼šä¸ä¼šå¸¦å‡ºå•¥è¿é”ååº”ã€‚

æ€’æƒ³åˆ«çš„åŠæ³•ã€‚è¿™ä¸ª [Github Issue](https://github.com/easydiffusion/easydiffusion/issues/1851) å¯ä»¥ç”¨ä½œå‚è€ƒï¼Œè§£å†³æ€è·¯å°±æ˜¯å» sentence_transformer å®‰è£…è„šæœ¬åœ¨ import çš„åœ°æ–¹æŠŠ cached_download æ”¹æˆæ–°çš„ hf_hub_downloadï¼ˆä½†ä¸åªè¿™ä¹ˆç®€å•ï¼‰ã€‚ä»è¿™ä¸ª issue é‡Œå¯ä»¥çœ‹åˆ°ï¼Œä¸åª sentence transformerï¼Œè¿˜æœ‰å…¶ä»–å¾ˆå¤šå°è£…äº† huggingface çš„å·¥å…·åº“çš„ä½¿ç”¨è€…/å¼€å‘è€…ä¹Ÿé‡åˆ°äº†è¿™ä¸ªé—®é¢˜ã€‚ä¸‹é¢æˆ‘å°±è®²ä¸€ä¸‹æ€ä¹ˆè‡ªå·±å»è§£å†³ã€‚

  1. æ ¹æ®ä¸€è¿ä¸²çš„æŠ¥é”™ä¿¡æ¯ï¼Œå®šä½åˆ°æœ‰ä¸¤ä¸ªå®‰è£…è„šæœ¬é‡ŒåŒ…å«äº†è¿™ä¸ªè¿‡æ—¶çš„ importï¼Œåˆ†åˆ«æ˜¯ `/home/ubuntu/<é¡¹ç›®æ–‡ä»¶å¤¹>/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py", line 12 ` ä»¥åŠç›¸åŒç›®å½•ä¸‹çš„ `util.py, line 17`ã€‚æ‰¾åˆ°ç›¸åº”ä½ç½®ï¼ŒæŠŠ import é‡Œé¢çš„ `cached_download` æ”¹æˆ `hf_hub_download`ã€‚
  2. å½“ç„¶ä¸åªè¿™ä¹ˆç®€å•ï¼`util.py` æ–‡ä»¶é‡Œé™¤äº† importï¼Œä»£ç é‡Œä¹Ÿè°ƒç”¨äº† `cached_download` è¿™ä¸ªå‡½æ•°ï¼Œå¹¶ä¸”è¿˜ä¼ å…¥äº† `cached_download_args` çš„ä¸€ç³»åˆ—å‚æ•°ã€‚å¾—æƒ³ä¸ªåŠæ³•è§£å†³è¿™ä¸€è¿ä¸²é—®é¢˜ã€‚

<figure>
  <div align=center><img src="/posts/tech/web-dynamic/cache-1.png"  style="width: 100%; height:auto;" alt="cached_download æŠ¥é”™"></div>
  <figcaption>æœ‰é—®é¢˜çš„åœ°æ–¹ï¼ˆä¿®æ”¹å‰ï¼‰</figcaption>
</figure>

  éœ€è¦æŠŠ path è¿™ä¸€è¡Œæ”¹æˆ `path = hf_hub_download(**cached_download_args) `ï¼Œä¹Ÿå°±æ˜¯æŠŠ `cached_download` æ›¿æ¢æˆ `hf_hub_download`ã€‚ç”±äº `cached_download_args`åªæ˜¯å‚æ•°çš„åå­—ï¼Œå‡½æ•°æ”¹åäº†å®ƒç…§æ ·å¯ä»¥ä¼ å‚ï¼Œæ‰€ä»¥ä¸ç”¨å»æ”¹å®ƒçš„åå­—ã€‚

  3. è¿˜æ˜¯æŠ¥é”™ï¼`TypeError: hf_hub_download() got an unexpected keyword argument 'url'` é—®é¢˜æ¥åˆ° `cached_download_args` å‚æ•°ã€‚æ—§çš„ `cached_download` æ¥å—çš„æ˜¯ `url` å‚æ•°ï¼ˆç›´æ¥ä¸‹è½½ URLï¼‰ï¼Œè€Œæ–°çš„ `hf_hub_download` å‡½æ•°ä¸å†ä¼ å…¥ `url` è¿™ä¸ªå‚æ•°ã€‚åŸå› å¯èƒ½æ˜¯ï¼š`hf_hub_download` è®¾è®¡çš„æ˜¯åŸºäºæ¨¡å‹ä»“åº“ï¼ˆrepo_id, filenameï¼‰çš„æ¥å£ï¼Œä¸æ”¯æŒ urlã€‚

  æ­¤æ—¶æŠŠè„šæœ¬å‘ä¸Šæ»‘ä¸€ç‚¹ï¼Œèƒ½çœ‹åˆ° `url` å‚æ•°çš„å®šä¹‰ã€‚æŒ‰ç…§ä¸‹å›¾çš„è¯´æ˜ï¼ŒæŠŠåŸæœ‰çš„ url é‡Œé¢çš„ä¸‰ä¸ªå‚æ•°æ‹†å‡ºæ¥ï¼Œæ›¿æ¢æ‰ `cached download_args` é‡Œé¢çš„ `url`ã€‚å¦å¤–ï¼Œ`legacy_cache_layout` è¿™ä¸ªå‚æ•°ä¹Ÿå¼ƒç”¨äº†ï¼Œä¼šæŠ¥é”™  `TypeError: hf_hub_download() got an unexpected keyword argument 'legacy_cache_layout'`ï¼Œå¦‚å›¾æŠŠè¿™ä¸€è¡Œæ³¨é‡Šæ‰å³å¯ã€‚

<figure>
  <div align=center><img src="/posts/tech/web-dynamic/cache-2.png"  style="width: 100%; height:auto;" alt="cached_download æŠ¥é”™2"></div>
  <figcaption>url å‚æ•°æ”¹å®Œä¹‹åçš„æ ·å­</figcaption>
</figure>

è‡³æ­¤ï¼Œè¿™ä¸ª `cached_download` å¼•å‘çš„ä¸€è¿ä¸²é—®é¢˜ç®—æ˜¯è§£å†³äº†ï¼Œåç»­å®‰è£…æ²¡æœ‰å†æŠ¥é”™ã€‚

## æ¸…ç†å†…å­˜çš„å‘½ä»¤
è¿™é‡Œè®°å½•ä¸€ä¸‹æ¸…ç†å†…å­˜çš„ä¸€äº›å‘½ä»¤ï¼Œæˆ‘çœŸå¿ƒå»ºè®®åˆ›å»ºå®ä¾‹çš„æ—¶å€™é€‰å¤§ä¸€ç‚¹çš„å­˜å‚¨å·ï¼Œè‡³å°‘ç•™ 12 Gï¼Œå¤šä¸äº†å¤šå°‘é’±ï¼Œä½“éªŒæ„Ÿå¤©å·®åœ°åˆ«ã€‚
```
# æ¸…ç†APTç¼“å­˜
sudo apt clean
sudo apt autoremove -y

# æ¸…ç†æ—¥å¿—æ–‡ä»¶
sudo find /var/log -name "*.log" -type f -delete
sudo find /var/log -name "*.gz" -type f -delete

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
sudo rm -rf /tmp/*
sudo rm -rf /var/tmp/*

# å†æ¬¡æ£€æŸ¥ç©ºé—´
df -h
```

## è¿œç¨‹ä¸»æœºå’Œæœ¬åœ°ä¸»æœºäº’ä¼ æ–‡ä»¶
WinSCPï¼ˆåç»­è¡¥å……ï¼‰

# åç«¯æœåŠ¡æ­å»º
æ¥åˆ°äº†é‡å¤´æˆï¼Œæ•´ä¸ª API æœåŠ¡é€»è¾‘å…¶å®éƒ½å†™åœ¨ app.py è¿™ä¸€ä¸ªæ–‡ä»¶é‡Œã€‚ç°åœ¨æ¥æ‹†è§£ä¸€ä¸‹å‡ ä¸ªä¸»è¦çš„éƒ¨åˆ†ã€‚

## FastAPI 
FastAPI æœ¬èº«ï¼Œå…¶å®ä»£ç ä¸å¤šã€‚

```python
# FastAPI çš„æœ¬ä½“ã€ä¾èµ–æ³¨å…¥å·¥å…·ï¼ˆç”¨äºDBè¿æ¥ã€è®¤è¯ã€å‚æ•°æ ¡éªŒï¼‰ã€æŠ›å‡ºé”™è¯¯ã€åŸºç¡€å“åº”ç±»ï¼ˆæ§åˆ¶è¿”å›å†…å®¹/çŠ¶æ€ç /headers/media_typeç­‰ï¼‰ã€æ¥è‡ªå®¢æˆ·ç«¯çš„è¯·æ±‚å¯¹è±¡
from fastapi import FastAPI, Depends, HTTPException, Response, Request 
# ä¸­é—´ä»¶ï¼Œé…ç½®è·¨åŸŸèµ„æºå…±äº«ï¼Œå…è®¸ç½‘é¡µè¯·æ±‚è½¬å‘åˆ°APIç«¯ç‚¹
from fastapi.middleware.cors import CORSMiddleware
# æ„é€  JSON å“åº”ã€æµå¼è¿”å›è¾“å‡º
from fastapi.responses import JSONResponse, StreamingResponse
# Pydantic å¯ä»¥è‡ªåŠ¨éªŒè¯è¾“å…¥æ•°æ®æ˜¯å¦ç¬¦åˆå®šä¹‰ï¼Œç”¨äºå¤„ç†ç”¨æˆ·è¾“å…¥ã€API äº¤äº’å’Œé…ç½®æ–‡ä»¶
from pydantic import BaseModel

# FastAPIåº”ç”¨åˆå§‹åŒ–
app = FastAPI(title="AI Blog API")

# Pydanticæ¨¡å‹
class QuestionRequest(BaseModel):
    question: str
```

## AI éƒ¨åˆ†

è¿™ä¸€éƒ¨åˆ†åŒ…æ‹¬ RAG çš„æµç¨‹å’Œ API çš„è°ƒç”¨é€»è¾‘ã€‚

### RAG
å®šä¹‰å¥½åµŒå…¥æ¨¡å‹å’Œå‘é‡åº“ï¼Œæ•´ç†å¥½è‡ªå·±çš„çŸ¥è¯†åº“ã€‚

#### åµŒå…¥æ¨¡å‹ä¸å‘é‡åº“
æˆ‘æœ€å¼€å§‹ç”¨çš„æ˜¯ sentence-transformers æä¾›çš„ [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) è¿™ä¸ªè¯åµŒå…¥æ¨¡å‹ã€‚æœ¬æ¥æ˜¯æ‰“ç®—ç”¨åˆšå¥½ä¸€å¥—çš„ sentence-transformer æ–¹æ³•æ¥åŠ è½½æ¨¡å‹ï¼Œç»“æœé‡åˆ°äº†å¾ˆå¤šæŠ¥é”™ï¼Œä»€ä¹ˆ CA è¯ä¹¦ã€model_type å‚æ•°é”™è¯¯ç­‰ç­‰ï¼Œäºæ˜¯éšæ‰‹è¯•äº†ä¸€ä¸‹ç›´æ¥ç”¨ Transformer æ¥åŠ è½½ï¼Œä¸€æ¬¡æˆåŠŸ......æ‰€ä»¥åˆšæ‰åœ¨é‚£è¾¹æ’æŸ¥äº†ä¸€å †é”™è¯¯ï¼Œæœ€åä¹Ÿæ²¡ç”¨ä¸Šã€‚

```python
# å…ˆé¢„å®šä¹‰é€šè¿‡TransformeråŠ è½½Embeddingæ¨¡å‹
class CustomEmbedder:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name, use_safetensors=True)
        self.model.eval()

    # å®šä¹‰å‡å€¼æ± åŒ–å‡½æ•°ï¼ŒæŠŠæ¯ä¸ª token çš„å‘é‡åˆå¹¶æˆæ•´å¥/æ–‡æœ¬çš„ä¸€ä¸ªå‘é‡
    def mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        # attention_mask æ ‡è®°å“ªäº› token æ˜¯å®é™…è¯ï¼ˆ1ï¼‰æˆ–å¡«å……ï¼ˆpaddingï¼Œ0ï¼‰
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        # æŠŠçœŸå® token çš„å‘é‡ç›¸åŠ ï¼Œç„¶åé™¤çœŸå® token çš„æ•°é‡ï¼Œå¾—åˆ°å¹³å‡å‘é‡ï¼Œä¹Ÿå°±æ˜¯å¥å­çš„å‘é‡è¡¨ç¤º
        # clamp min 1e-9 æ˜¯ä¸ºäº†é˜²æ­¢é™¤ä»¥ 0ï¼ˆæç«¯æƒ…å†µæ•´å¥å…¨æ˜¯ paddingï¼‰
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    # æŠŠè¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæˆå¥å‘é‡ï¼Œæ–¹ä¾¿åç»­åšå‘é‡æ£€ç´¢
    def encode(self, texts):
        if isinstance(texts, str):
            texts = [texts]
        # ç”¨ tokenizer å°†æ–‡æœ¬ç¼–ç æˆæ¨¡å‹è¾“å…¥ï¼Œè¿”å› PyTorch å¼ é‡ï¼ˆtensorï¼‰
        inputs = self.tokenizer(texts, padding=True, truncation=True, 
                               return_tensors='pt', max_length=512)
        # è¿è¡Œæ¨ç†ï¼Œå¾—åˆ°æ¯ä¸ª token çš„å‘é‡è¾“å‡º
        with torch.no_grad():
            outputs = self.model(**inputs)
        # è°ƒç”¨åˆšæ‰å®šä¹‰å¥½çš„æ± åŒ–å‡½æ•°ï¼Œæ ¹æ® attention mask å¿½ç•¥ padding tokenï¼Œåªå¯¹çœŸå® token çš„å‘é‡è®¡ç®—åµŒå…¥
        embeddings = self.mean_pooling(outputs, inputs['attention_mask'])
        # å¯¹æ¯ä¸ªå‘é‡åš L2 å½’ä¸€åŒ–ï¼ˆä¾¿äºç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ¯”è¾ƒï¼‰ï¼Œç„¶åè½¬æ¢ä¸º numpy æ•°ç»„è¿”å›
        return F.normalize(embeddings, p=2, dim=1).numpy()

# å…¨å±€å˜é‡
embedder = CustomEmbedder()  # åµŒå…¥æ¨¡å‹åˆå§‹åŒ–ä¸ºåˆšæ‰é¢„å®šä¹‰å¥½çš„Embeddingæ¨¡å‹
chroma_client = chromadb.PersistentClient(path="./chroma_db") #åˆå§‹åŒ– ChromaDB å‘é‡æ•°æ®åº“ï¼Œæ•°æ®å°†ä¿å­˜åœ¨å½“å‰ç›®å½•çš„chroma_dbæ–‡ä»¶å¤¹ä¸­
collection = chroma_client.get_or_create_collection("personal_knowledge") # è·å–æˆ–åˆ›å»ºçŸ¥è¯†åº“é›†åˆ
DOCUMENTS_DIR = "/home/ubuntu/<é¡¹ç›®ç›®å½•>/rag_documents" # å­˜æ”¾çŸ¥è¯†åº“æ–‡ä»¶çš„ç›®å½•
```

#### æ„å»º RAG
åˆ©ç”¨åˆšæ‰æ­å»ºå¥½çš„ embedding å’Œå‘é‡åº“ï¼Œå¼€å§‹æ­å»º RAG æµç¨‹ã€‚

```python
# æ„å»ºæˆ–æ›´æ–°RAGçŸ¥è¯†åº“
def build_knowledge_base():
    # å£°æ˜ global å…¨å±€å˜é‡ï¼Œåœ¨å‡½æ•°é‡Œæ›´æ–°åï¼Œå…¨å±€çš„ collection ä¹Ÿä¼šæ›´æ–°
    global collection

    try:
        # å°è¯•åˆ é™¤ç°æœ‰é›†åˆï¼Œé¿å…æ¯æ¬¡çŸ¥è¯†åº“æ›´æ–°åè¿˜åŒ¹é…åˆ°æ—§çš„
        chroma_client.delete_collection("personal_knowledge")
    except:
        pass  # å¦‚æœé›†åˆä¸å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯

    collection = chroma_client.get_or_create_collection("personal_knowledge")

    all_docs = []
    all_embeddings = []
    all_ids = []

    # æ”¯æŒä¸¤ç§æ–‡ä»¶æ ¼å¼
    supported_extensions = ('*.pdf', '*.docx')
    files_to_process = []
    for ext in supported_extensions:
        files_to_process.extend(glob.glob(os.path.join(DOCUMENTS_DIR, ext)))

    # è¿™æ˜¯ RAG çš„å‚æ•°ï¼Œåç»­å¯ä»¥æ ¹æ®æ•ˆæœä¼˜åŒ–
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°
        chunk_overlap=50  # å—ä¹‹é—´çš„é‡å éƒ¨åˆ†
    )

    doc_id = 0
    for file_path in files_to_process:
        try:
            if file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            elif file_path.endswith('.pdf'):
                # éœ€è¦å®‰è£… PyPDF2: pip install pypdf2
                import PyPDF2
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    text = "\n".join([page.extract_text() for page in pdf_reader.pages])
            elif file_path.endswith('.docx'):
                # å¤„ç† DOCX æ–‡ä»¶
                from docx import Document
                doc = Document(file_path)
                text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            else:
                continue  # æš‚æ—¶è·³è¿‡å…¶ä»–æ ¼å¼

            # åˆ†å‰²æ–‡æœ¬
            chunks = text_splitter.split_text(text)

            for chunk in chunks:
                # ç”ŸæˆåµŒå…¥å‘é‡
                embedding = embedder.encode(chunk).tolist()[0] 
                all_docs.append(chunk)
                all_embeddings.append(embedding)
                all_ids.append(f"doc_{doc_id}")
                doc_id += 1

        except Exception as e:
            print(f"Error processing {file_path}: {str(e)}")
            continue

    # æ‰¹é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
    if all_docs:
        collection.add(
            documents=all_docs,
            embeddings=all_embeddings,
            ids=all_ids
        )
        print(f"Successfully loaded {len(all_docs)} document chunks into knowledge base.")
    else:
        print("No documents were processed.")

# åŠ ä¸€ä¸ªæ¯æ¬¡å¯åŠ¨APIæœåŠ¡æ—¶ï¼Œè‡ªåŠ¨æ„å»ºçŸ¥è¯†åº“çš„å‡½æ•°
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    print("Building knowledge base...")
    build_knowledge_base()
    print("Knowledge base built successfully.")
```

#### RAGè°ƒä¼˜
è¿™éƒ¨åˆ†æ˜¯å…¨éƒ¨æœåŠ¡éƒ½éƒ¨ç½²å®Œæˆã€æµ‹è¯•åï¼Œå‘ç°æ¨¡å‹å›ç­”æ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œæ‰€ä»¥è¿”å›æ¥è¡¥å……çš„ã€‚æµ‹è¯•æ—¶ï¼Œæˆ‘æä¸€ä¸ªç­”æ¡ˆå¾ˆå¥½æ‰¾çš„é—®é¢˜ï¼Œå¾ˆæ˜æ˜¾åœ°å†™åœ¨çŸ¥è¯†åº“é‡Œäº†ï¼Œä½†æ˜¯ç­”æ¡ˆå°±æ˜¯ä¸æé‚£éƒ¨åˆ†ã€‚è€Œä¸”é—®æ¥é—®å»ï¼Œä¼¼ä¹æ¨¡å‹éƒ½åªä¼šå›ç­”æˆ‘â€œé¡¹ç›®ç»å†â€é‚£ä¸€æ®µå†…å®¹ï¼Œå°±ç®—é—®é¢˜ä¸å¤ªç›¸å…³ï¼Œå®ƒä¹Ÿä¼šæ‰¯åˆ°è¿™ä¸Šé¢ï¼Œå¯èƒ½ RAG åµŒå…¥+æ£€ç´¢çš„æ—¶å€™åªè¯†åˆ«åˆ°äº†é‚£ä¸€æ®µä¿¡æ¯ã€‚

RAG è°ƒä¼˜å‡ ä¸ªä¸»è¦çš„æ–¹æ³•ï¼š
1. Embedding æ¨¡å‹æœ¬èº«çš„è°ƒæ•´ã€‚
2. è°ƒå‚ï¼Œå‚æ•°åŒ…æ‹¬ Chunking åˆ‡å—çš„ token æ•°ã€Overlap é‡å çª—å£çš„å¤§å°ã€Top-k é€‰å–å‰å¤šå°‘ä¸ªæ£€ç´¢åŒ¹é…ç»“æœã€‚å…¶ä¸­ Chunking é™¤äº†æŒ‰ token æ•°æ¥åˆ‡å—å¤–ï¼Œè¿˜æœ‰å…¶ä»–çš„æ–¹å¼ï¼Œå¦‚æŒ‰ç…§æ®µè½æˆ–è¯­ä¹‰åˆ‡å—ã€‚
3. Rerank é‡æ’ï¼Œindexing ç´¢å¼•ä¼˜åŒ–ï¼Œquery rewrite æŸ¥è¯¢é‡å†™ã€‚

æŸ¥åˆ°äº†è¿™äº›æ–¹æ³•ä¹‹åï¼Œå‡†å¤‡ä¸€æ¡æ¡å°è¯•ã€‚æˆ‘è¿˜åœ¨åç«¯ä»£ç é‡Œåšäº†åŸ‹ç‚¹ï¼ŒåŠ äº†ä¸€ä¸ª config å‚æ•°ï¼Œè·Ÿç€é—®ç­”è®°å½•ä¸€èµ·å±•ç¤ºåˆ° Grafana çš„ SQL é‡Œï¼Œæ–¹ä¾¿åæœŸæµ‹è¯•ä¸åŒçš„è°ƒä¼˜æ–¹æ³•å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ã€‚

ç›®å‰å°è¯•äº†ç¬¬1ä¸ªæ–¹æ³•â€”â€”æ”¹Embeddingæ¨¡å‹ã€‚èµ·å› æ˜¯æˆ‘ä¹‹å‰ç”¨çš„å°ºå¯¸å¾ˆå°çš„æ¨¡å‹ all-MiniLM-L6-v2 çš„ Huggingface è¯„è®ºåŒºï¼Œæœ‰äººé—®ï¼šä¸­æ–‡è¡¨ç°å’‹æ ·ï¼Ÿä¸€è‡´åé¦ˆæ˜¯ï¼šä¸å’‹æ ·â€”â€”æˆ‘ä¹Ÿè§‰å¾—ï¼ä¹‹å‰åªå…³æ³¨æ¨¡å‹å°ºå¯¸äº†ï¼Œæ²¡å»æƒ³è¯­è¨€çš„é—®é¢˜ã€‚åˆæ‰¾äº†å‡ ä¸ªç»è¿‡ä¸­æ–‡é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œæœ€åé”å®šåœ¨ DMetaSoul/sbert-chinese-general-v2-distill è¿™ä¸ªæ¨¡å‹ï¼Œå°ºå¯¸åŒæ ·å¾ˆå°ï¼Œæ–¹ä¾¿éƒ¨ç½²åœ¨æˆ‘çš„ä¸ç‰ˆä¸»æœºä¸Šã€‚

åœ¨åç«¯ä»£ç é‡Œé¢æ”¹äº†æ¨¡å‹ï¼Œä¸å¤¸å¼ åœ°è¯´ï¼Œä¸€ä¸‹å°±å˜èªæ˜äº†ï¼Œå¯¹ç­”å¦‚æµä¸”å‡»ä¸­è¦ç‚¹ï¼è¿™è¿˜æ˜¯æ²¡æœ‰ç»§ç»­å…¶ä»– RAG è°ƒä¼˜æ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å·²ç»æå‡äº†å¾ˆå¤šã€‚ç”±äºåç»­è¿˜æœ‰å…¶ä»–çš„å·¥ä½œè¦åšï¼Œæš‚æ—¶æ²¡æœ‰å†å¼„å…¶ä»–å‡ ç§è°ƒä¼˜ï¼Œåé¢å¯ä»¥è¿”å›æ¥ç»§ç»­ã€‚

### Deepseek API
API è°ƒç”¨ä¸»è¦æ˜¯ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªè´Ÿè´£ä¸ Deepseek æ¥å£äº¤äº’ï¼Œä¸€ä¸ªè´Ÿè´£é—®ç­”çš„å†…éƒ¨å¤„ç†é€»è¾‘å’Œè·¯ç”±ã€‚

```python
def get_ai_response(prompt):
    """å‘èµ·HTTPè¯·æ±‚è°ƒç”¨Deepseek APIè·å–å›ç­”"""
    api_key = os.getenv("DEEPSEEK_API_KEY")  # æ­¤å¤„éœ€è¦æå‰æ³¨å†Œ Deepseek å¼€å‘è€…è´¦å·ï¼Œè·å–å¯†é’¥ï¼Œå­˜åˆ° os çš„ç¯å¢ƒå˜é‡ä¸­
    if not api_key:
        raise ValueError("Deepseek API key not found in environment variables")

    url = "https://api.deepseek.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "deepseek-chat",  # æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©æ¨¡å‹
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
        "stream": True
    }

    try:
        with requests.post(url, json=data, headers=headers, stream=True, timeout=30) as r:
            r.raise_for_status()
            for line in r.iter_lines(decode_unicode=True):
                if line:
                    yield line + "\n"  # æ¯è¡Œé€å—å‘é€
    except requests.exceptions.RequestException as e:
        yield f"data: {json.dumps({'error': f'Deepseek API error: {str(e)}'})}\n\n"

# --- APIè·¯ç”± endpointï¼Œå°†å‡½æ•°æš´éœ²ä¸º POST æ¥å£--- 
@app.post("/api/ask_stream")
async def ask_stream(request: QuestionRequest, db: Session = Depends(get_db)):
    try:
        # 1. å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºå‘é‡
        question_embedding = embedder.encode(request.question).tolist()
        if isinstance(question_embedding[0], float):
            question_embedding = [question_embedding]  # åŒ…è£…æˆäºŒç»´åˆ—è¡¨

        # 2. åœ¨å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£
        results = collection.query(
            query_embeddings=question_embedding,
            n_results=3  # æ£€ç´¢Top3æœ€ç›¸å…³çš„ç‰‡æ®µï¼Œè¿™ä¹Ÿå°±æ˜¯ RAG ä¼˜åŒ–ä¸­çš„ top-k å‚æ•°
        )
        retrieved_docs = results['documents'][0] if results['documents'] else ["No relevant context found."]

        # 3. æ„å»ºPrompt
        context_str = "\n".join(retrieved_docs)
        prompt = f"""æˆ‘çš„åå­—å«ç†Šç†Šï¼Œä½ æ˜¯æˆ‘çš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„å…³äºæˆ‘çš„èƒŒæ™¯ä¿¡æ¯æ¥å¸®æˆ‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

        ã€èƒŒæ™¯ä¿¡æ¯ã€‘
        {context_str}

        ã€ç”¨æˆ·é—®é¢˜ã€‘
        {request.question}

        ã€å›ç­”è¦æ±‚ã€‘
        1. å‡è£…ä½ å°±æ˜¯æˆ‘ï¼Œå…¨éƒ¨ä½¿ç”¨ç¬¬ä¸€äººç§°è§†è§’å›ç­”ï¼ˆå¦‚"æˆ‘æ›¾ç»..."ï¼‰ã€‚
        2. åŸºäºæˆ‘çš„èƒŒæ™¯ä¿¡æ¯ä¸­çš„å†…å®¹å›ç­”é—®é¢˜ã€‚
        3. å¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸åŒ…å«ç­”æ¡ˆï¼Œè¯·è¡¨è¾¾æ ¹æ®å·²æœ‰ä¿¡æ¯ä½ æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œä½†å¯ä»¥ä½¿ç”¨ç”ŸåŠ¨çš„è¯­æ°”ã€‚
        4. é£æ ¼è‡ªç„¶äº²åˆ‡ï¼Œä½†ä¿æŒä¸“ä¸šã€‚
        5. ä¸è¦ç¼–é€ èƒŒæ™¯ä¿¡æ¯ä¸­ä¸å­˜åœ¨çš„å†…å®¹ã€‚
        """

        # è®°å½•æˆåŠŸçš„è¯·æ±‚
        API_REQUEST_COUNT.labels(method='POST', endpoint='/ask_stream', status='200').inc()
        # return JSONResponse(content={"question": request.question, "answer": answer, "id": record.id}) è¿™ä¸ªæ˜¯æœ€å¼€å§‹å°è¯•çš„æ•´æ®µè¿”å›å›ç­”ï¼Œåæ¥å‘ç°å‰ç«¯ç”¨æˆ·ç­‰å¾…æ—¶é—´å¤ªé•¿ï¼Œæ‰€ä»¥æ”¹ä¸ºæµå¼ä¼ è¾“ï¼Œç”Ÿæˆäº†å¤šå°‘å°±å…ˆè¾“å‡ºï¼Œä¸å¿…ç­‰åˆ°ç”Ÿæˆå®Œä¸€æ•´æ®µå†ä¸€èµ·è¿”å›åˆ°å‰ç«¯
        return StreamingResponse(get_ai_response(prompt), media_type="text/event-stream")

    except Exception as e:
        # è®°å½•å¤±è´¥çš„è¯·æ±‚
        API_REQUEST_COUNT.labels(method='POST', endpoint='/ask_stream', status='500').inc()
        raise HTTPException(status_code=500, detail=str(e))
```

## ç½‘ç»œå’ŒNginx

ç”±äºè¿™ä¸ª API æ˜¯ä½äºæ¬¡çº§åŸŸåä¸‹ï¼ˆåŸåŸŸåå‰é¢åŠ ä¸Šâ€œapiâ€ï¼‰ï¼Œä¸å‰ç«¯é¡µé¢ä¸èƒ½ç®—å±äºåŒä¸€ä¸ª originï¼Œä¼šè¢«æµè§ˆå™¨çš„åŒæºç­–ç•¥æ‹¦æˆªï¼Œé˜»æ­¢å‰ç«¯ JS è„šæœ¬ç›´æ¥å‘å‡ºè·¨åŸŸè¯·æ±‚æˆ–è¯»å–å“åº”ã€‚æƒ³è¦ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡å‰ç«¯äº¤äº’æŠŠè¯·æ±‚å‘åˆ°åç«¯å¤„ç†ï¼Œå°±å¿…é¡»é…ç½®è·¨åŸŸç­–ç•¥ CORSã€‚

```python
# å…è®¸å‰ç«¯è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:1313",    # æœ¬åœ°Hugoå¼€å‘æœåŠ¡å™¨
        "https://bearlybear.com",   # ç”Ÿäº§åŸŸå
        "https://www.bearlybear.com"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

Nginx æ–‡ä»¶å­˜æ”¾åœ¨ /etc/nginx/sites-available/<é¡¹ç›®å>ï¼Œç”¨äºæ¥å—å‰ç«¯çš„è¯·æ±‚ï¼ŒæŠŠè¯·æ±‚è½¬å‘åˆ°åç«¯ï¼Œé¿å…åç«¯æš´éœ²ã€‚
```Nginx
server {
    listen 80; # ç›‘å¬80ç«¯å£ï¼Œå³http
    server_name api.bearlybear.com; 
    return 301 https://$host$request_uri; # æŠŠhttpè¯·æ±‚é‡å®šå‘åˆ°https
}

server {
    listen 443 ssl; # ç›‘å¬443ç«¯å£ï¼Œå³httpsï¼Œå¹¶å¼€å¯TLSåŠ å¯†
    server_name api.bearlybear.com;
    # æŒ‡å®šTLSè¯ä¹¦å’Œç§é’¥æ–‡ä»¶ï¼Œè¿™ä¸ªåœ¨ä¸‹é¢ä¸€æ®µä¼šè®²å¦‚ä½•é…ç½®
    ssl_certificate /etc/ssl/certs/bearlybear-origin.pem;
    ssl_certificate_key /etc/ssl/private/bearlybear-origin.key;
    
    # APIè½¬å‘ï¼ŒåŒ¹é…ä»¥/api/å¼€å¤´çš„è·¯å¾„ï¼Œæ¯”å¦‚/api/ask_stream
    location /api/ {
        proxy_pass http://127.0.0.1:8000; # è¯·æ±‚è½¬å‘åˆ°åç«¯æœåŠ¡
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # ç›‘æ§æŒ‡æ ‡ï¼Œä¼šè¢« Prometheus æŠ“å–ã€‚è¿™éƒ¨åˆ†è¿˜éœ€è¦åœ¨ app.py åç«¯é‡Œé¢å®šä¹‰ï¼Œè¯¦æƒ…æˆ‘å†™åœ¨ Prometheus é‚£éƒ¨åˆ†äº†
    location /metrics {
        proxy_pass http://127.0.0.1:8000/metrics;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}

```
ä¿®æ”¹å®Œæˆåï¼Œå¯ä»¥ç”¨è¿™ä¸ªå‘½ä»¤æ£€æŸ¥ Nginx è¯­æ³•ï¼š
```shell
sudo nginx -t
```

ç¡®è®¤æ— è¯¯åï¼Œå¯ç”¨é…ç½®å¹¶é‡å¯ Nginxï¼š
```shell
sudo ln -s /etc/nginx/sites-available/<é¡¹ç›®å> /etc/nginx/sites-enabled/
sudo systemctl restart nginx
```

è¿™é‡Œè¿˜éœ€è¦ç»™ API çš„å­åŸŸåé…ç½®ä¸€ä¸ª HTTPS è¯ä¹¦ã€‚æ­¤å‰æˆ‘åªæœ‰é™æ€åŸŸåçš„è¯ä¹¦ï¼Œè¿™ä¸ªå­åŸŸåæ— æ³•å…±äº«ï¼Œæ‰€ä»¥éœ€è¦é‡æ–°é…ç½®ä¸€ä¸ªã€‚å› ä¸ºåŸŸåç®¡ç†æˆ‘ç”¨çš„æ˜¯ Cloudflareï¼Œæ‰€ä»¥ç›´æ¥ç»§ç»­ç”¨ Cloudflare çš„ Origin è¯ä¹¦ã€‚è¿›å…¥ Cloudflare æ§åˆ¶å° >  SSL/TLS > Origin Serverï¼Œæ–°å»ºè¯ä¹¦ï¼Œä¸»æœºåéœ€è¦åŒ…å«æˆ‘çš„æ‰€æœ‰å­åŸŸåï¼š

> bearlybear.com <br/>
> *.bearlybear.com (é€šé…ç¬¦ï¼Œè¦†ç›–æ‰€æœ‰å­åŸŸå) <br/>
> api.bearlybear.com

å…¶ä»–é€‰é¡¹å¯ä¿æŒé»˜è®¤ã€‚å®Œæˆè¯ä¹¦åˆ›å»ºåï¼Œä¼šå¾—åˆ°ä¸¤ä¸ªæ–‡ä»¶ï¼šè¯ä¹¦æ–‡ä»¶ (.pem æ ¼å¼)å’Œç§é’¥æ–‡ä»¶ (.pem æ ¼å¼)ã€‚å°†è¿™ä¸¤ä¸ªæ–‡ä»¶çš„å†…å®¹å¤åˆ¶ä¿å­˜ä¸‹æ¥ã€‚

ä¸‹ä¸€æ­¥ï¼Œå›åˆ° EC2 æœ¬èº«é…ç½® SSL è¯ä¹¦ã€‚
```shell
# åˆ›å»ºSSLè¯ä¹¦ç›®å½•
sudo mkdir -p /etc/ssl/certs
sudo mkdir -p /etc/ssl/private
# è®¾ç½®æ­£ç¡®çš„æƒé™
sudo chmod 700 /etc/ssl/private
# åˆ›å»ºè¯ä¹¦æ–‡ä»¶ï¼ŒæŠŠåˆšæ‰å¤åˆ¶çš„è¯ä¹¦å†…å®¹æ”¾è¿›æ–‡ä»¶é‡Œ
sudo nano /etc/ssl/certs/bearlybear-origin.pem
# åˆ›å»ºç§é’¥æ–‡ä»¶ï¼ŒæŠŠåˆšæ‰å¤åˆ¶çš„ç§é’¥å†…å®¹æ”¾è¿›æ–‡ä»¶é‡Œ
sudo nano /etc/ssl/private/bearlybear-origin.key
# è®¾ç½®ä¸¥æ ¼çš„æƒé™
sudo chmod 600 /etc/ssl/private/bearlybear-origin.key
sudo chmod 644 /etc/ssl/certs/bearlybear-origin.pem
sudo chown root:root /etc/ssl/private/bearlybear-origin.key
```
ç›¸åº”åœ°ï¼Œåœ¨ Nginx é…ç½®æ–‡ä»¶é‡Œä¹Ÿéœ€è¦æŒ‡å®šè¿™ä¸¤ä¸ªå¯†é’¥æ–‡ä»¶ï¼Œè¯¦è§ä¸Šé¢ Nginx æ–‡ä»¶å†…å®¹ã€‚å®Œæˆåï¼ŒAPI æœåŠ¡ä¹Ÿè·Ÿé™æ€ç½‘é¡µä¸€æ ·å—åˆ° SSL ä¿æŠ¤äº†ï¼Œæ¯” HTTP æ›´å®‰å…¨ã€‚

## API Service æ–‡ä»¶

åˆ°è¿™é‡Œï¼Œapp.py é‡Œé¢çš„å†…å®¹å·²ç»åŸºæœ¬å®Œæˆäº†ï¼Œè¦è®©å®ƒèƒ½å¤ŸæŒç»­è¿è¡Œåœ¨ä¸»æœºä¸Šï¼Œè¿˜éœ€è¦æŠŠå®ƒåŒ…è£…æˆä¸€ä¸ªæœåŠ¡ï¼ˆserviceï¼‰ï¼Œ

```
sudo nano /etc/systemd/system/<é¡¹ç›®å>.service

[Unit]
Description=AI Blog API Service
After=network.target

[Service]
User=ubuntu
Group=ubuntu
WorkingDirectory=/home/ubuntu/<é¡¹ç›®ç›®å½•>
Environment="PATH=/home/ubuntu/<é¡¹ç›®ç›®å½•>/venv/bin:/usr/bin:/bin"
Environment="DEEPSEEK_API_KEY=è¿™ä¸ªé‡Œé¢å¡«APIçš„å¯†é’¥"
Environment="ANONYMIZED_TELEMETRY=false"
ExecStart=/home/ubuntu/<é¡¹ç›®å>/venv/bin/uvicorn app:app --host 127.0.0.1 --port 8000
Restart=on-failure
RestartSec=10s
StandardOutput=journal
StandardError=journal
StartLimitIntervalSec=60
StartLimitBurst=3
TimeoutStartSec=45s
TimeoutStopSec=15s
MemoryMax=2G
CPUQuota=80%

[Install]
WantedBy=multi-user.target
```

é…ç½®å¥½åï¼Œç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨æœåŠ¡ï¼š
```shell
# é‡æ–°åŠ è½½systemdé…ç½®
sudo systemctl daemon-reload
# å¯åŠ¨æœåŠ¡
sudo systemctl start <æœåŠ¡å>
# è®¾ç½®å¼€æœºè‡ªå¯
sudo systemctl enable <æœåŠ¡å>
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
sudo systemctl status <æœåŠ¡å>
# æŸ¥çœ‹æ—¥å¿—ï¼ˆå¦‚æœæœ‰é—®é¢˜ï¼‰
sudo journalctl -u <æœåŠ¡å> -f
```

åˆ°è¿™é‡Œä¸ºæ­¢ï¼Œè¿™ä¸ªçš„ API æœåŠ¡å°±é…ç½®å®Œæˆäº†ã€‚å¯ä»¥ç”¨ cURL æ¥æµ‹è¯•æ˜¯å¦è¯·æ±‚æ˜¯å¦èƒ½é€šã€‚

```shell
curl -X POST "http://<EC2çš„IP>:8000/api/ask-stream" \
     -H "Content-Type: application/json" \
     -d '{"question": "ä½ æ˜¯è°ï¼Ÿ"}'
```


## Prometheus 
åœ¨ app.py åç«¯ä¸­ï¼Œè¿˜éœ€è¦æš´éœ²ä¸€äº›ç›‘æ§ç»™ Prometheus æ¥æŠ“å–ï¼Œä»¥ä¾¿åç»­å±•ç¤ºåœ¨ Grafana ä¸­ã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬ API è¯·æ±‚é‡ã€è¯·æ±‚æ—¶å»¶ç­‰ã€‚

```python
import prometheus_client
from prometheus_client import Counter, Histogram, generate_latest, REGISTRY, CONTENT_TYPE_LATEST
from prometheus_client.exposition import CONTENT_TYPE_LATEST
from prometheus_client import PROCESS_COLLECTOR, PLATFORM_COLLECTOR

# åˆå§‹åŒ–é»˜è®¤æŒ‡æ ‡
PROCESS_COLLECTOR  # ç¡®ä¿è¿›ç¨‹æ”¶é›†å™¨è¢«å¯¼å…¥
PLATFORM_COLLECTOR  # ç¡®ä¿å¹³å°æ”¶é›†å™¨è¢«å¯¼å…¥
prometheus_client.start_http_server(0)  # ç«¯å£0è¡¨ç¤ºä¸å¯åŠ¨é¢å¤–æœåŠ¡å™¨

# --- Prometheusç›‘æ§æŒ‡æ ‡å®šä¹‰ ---
API_REQUEST_COUNT = Counter('api_request_total', 'Total API requests', ['method', 'endpoint', 'status'])
API_REQUEST_DURATION = Histogram('api_request_duration_seconds', 'API request duration', ['endpoint'])

@app.get("/metrics")
async def metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    try:
        from prometheus_client import generate_latest
        data = generate_latest()
        return Response(
            content=data,
            media_type=CONTENT_TYPE_LATEST
        )
    except Exception as e:
        logger.error(f"Metrics endpoint error: {e}")
        raise HTTPException(status_code=500, detail="Metrics generation failed")

# è¿™ä¸ªå…¶å®ä¸ç”¨ Prometheus æ¥æŠ“å–ï¼Œå¯ä»¥ç›´æ¥ç”¨ curl æ£€æŸ¥çŠ¶æ€
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "service": "AI Blog API"}
```

Prometheus æœ¬èº«çš„é…ç½®åœ¨ prometheus.yml è¿™ä¸ªæ–‡ä»¶é‡Œã€‚
```yml
# å‰é¢çœç•¥é»˜è®¤çš„ä¸€äº›é…ç½®
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "prometheus"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9090"]
       # The label name is added as a label `label_name=<label_value>` to any timeseries scraped from this config.
        labels:
          app: "prometheus"
  
  # ä¸‹é¢å¼€å§‹æ–°å¢è‡ªå®šä¹‰çš„æŒ‡æ ‡
  - job_name: 'api-job' # ä¸€ä¸ªæ–°çš„ä½œä¸šï¼Œç”¨äºç›‘æ§APIæœåŠ¡
    static_configs:
      - targets: ['localhost:8000'] # APIæœåŠ¡ç«¯å£
    metrics_path: '/metrics' # metricsç«¯ç‚¹è·¯å¾„
    scrape_interval: 15s # æŠ“å–é—´éš”

  - job_name: 'node-exporter' # æŠ“å–ä¸»æœºå¥åº·æŒ‡æ ‡
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:9100']
```

æœ€åè¿™ä¸ª node-exporter æ˜¯ Prometheus æä¾›çš„ä¸€ä¸ªå·¥å…·ï¼Œç”¨æ¥æ”¶é›† EC2 ä¸»æœºæœ¬èº«çš„è¿è¡ŒæŒ‡æ ‡ï¼Œæ¯”å¦‚ CPU åˆ©ç”¨ç‡ã€å†…å­˜ç­‰ï¼Œéœ€è¦æå‰ä¸‹è½½å¹¶è§£å‹ã€‚å®ƒçš„ç«¯å£å·æ˜¯ 9100ï¼Œä¼šæš´éœ²ç»™ Prometheus æ¥æŠ“å–æ•°æ®ã€‚å¯¹äº node-exporterï¼Œå»ºè®®ä¹Ÿé…ç½®ä¸€ä¸ªsystemdæ–‡ä»¶ï¼Œä¾¿äºå¼€æœºè‡ªå¯ï¼š
```shell
sudo nano /etc/systemd/system/node-exporter.service

[Unit]
Description=Node Exporter
After=network.target

[Service]
Type=simple
User=root
ExecStart=/usr/local/bin/node_exporter --web.listen-address=127.0.0.1:9100
Restart=on-failure
RestartSec=30
StartLimitBurst=5

NoNewPrivileges=yes
PrivateTmp=yes

[Install]
WantedBy=multi-user.target
```

é…ç½®å®Œæˆåï¼Œé‡å¯æœåŠ¡ï¼Œè·Ÿåˆšæ‰ Nginx çš„æµç¨‹ä¸€æ ·ï¼Œç”¨ systemctl å‘½ä»¤ daemon-reload ç„¶å enable æœ€å start å³å¯ã€‚

è¦æµ‹è¯•æŒ‡æ ‡æ˜¯å¦èƒ½è¢«æˆåŠŸæŠ“å–ï¼š
```shell
# Prometheus æŒ‡æ ‡
curl -v http://localhost:8000/metrics

# æ£€æŸ¥ node-exporter ç«¯å£ç›‘å¬
sudo netstat -tlnp | grep 9100 

# node-exporter æµ‹è¯•æŒ‡æ ‡æ”¶é›†
curl http://localhost:9100/metrics | grep -E "(node_memory|node_cpu)"
```


## Grafana
ä¸Šé¢åœ¨ Prometheus ä¸­é…ç½®å®Œå„é¡¹æŠ“å–æŒ‡æ ‡åï¼Œå…¶å®åœ¨ Prometheus é¡µé¢æœ¬èº«ä¹Ÿå¯ä»¥ç®€å•æŸ¥è¯¢æŒ‡æ ‡ï¼Œä½†æ˜¯å¦‚æœæƒ³åšæ›´åŠ ç›´è§‚çš„å¯è§†åŒ–é¢æ¿ï¼Œå°±è¦ç”¨åˆ° Grafanaï¼Œå®ƒæ”¯æŒå¯¼å…¥å¤šç§æ•°æ®æºï¼Œå¹¶å¯è§†åŒ–ä¸ºè¡¨æ ¼ã€é¥¼å›¾ã€æŠ˜çº¿å›¾ã€æŸ±çŠ¶å›¾ç­‰ç­‰ç±»å‹ã€‚

é¦–å…ˆï¼Œåœ¨å®ä¾‹ä¸Šå®‰è£…å¹¶å¯åŠ¨ Grafanaã€‚

```shell
sudo apt-get install -y grafana
sudo systemctl start grafana-server
sudo systemctl enable grafana-server
```

ç„¶åå°±å¯ä»¥é€šè¿‡ç½‘é¡µè®¿é—®GUIäº†ï¼Œåœ°å€æ˜¯ http://\<EC2-IP>:3000ï¼Œé»˜è®¤ç”¨æˆ·å/å¯†ç ï¼šadmin/adminã€‚

### å¯è§†åŒ–é¢æ¿é…ç½®
é¢æ¿ä¹Ÿå°±æ˜¯ Dashboardï¼Œåœ¨é¡µé¢ä¸­ç‚¹å‡»æ–°å»º Dashboardã€‚åœ¨è¿™é‡Œæˆ‘å»ºäº†3ä¸ªé¢æ¿ï¼Œåˆ†åˆ«å±•ç¤ºä¸»æœºå¥åº·çŠ¶æ€ã€APIè°ƒç”¨æ¬¡æ•°å’Œé¢‘ç‡ï¼Œä»¥åŠAPIæé—®å’Œå›ç­”çš„æ–‡å­—è®°å½•ã€‚åœ¨ Explore èœå•æ é‡Œæ·»åŠ æ•°æ®æºï¼Œç„¶ååœ¨åˆ›å»ºé¢æ¿çš„æ—¶å€™é€‰æ‹©éœ€è¦çš„æ•°æ®æºï¼Œå¹¶ä¸”è®¾ç½®ç›¸åº”çš„æŠ“å–ä»£ç ã€‚ï¼ˆå¾…è¡¥å……ï¼‰


### SQLite
æˆ‘ç”¨ SQLite æ¥è®°å½•ç”¨æˆ·çš„AIé—®ç­”å†å²æ–‡å­—ã€‚
```python
from sqlalchemy.orm import Session
from db import get_db, SessionLocal, init_db, QAHistory

```

# å‰ç«¯ä»£ç ä¿®æ”¹

## è§†è§‰å…ƒç´ 
æœ¬æ¥æŒ‰ç…§ Hugo çš„ä¸»é¢˜æ¨¡æ¿ï¼Œé¦–é¡µåªæœ‰å››ä¸ªå…¥å£ã€‚ç°åœ¨ä¸ºäº†å¢åŠ  AI å¯¹è¯å…¥å£ï¼Œéœ€è¦æ–°å¢ä¸€ä¸ªé•¿æ–¹å½¢çš„æŒ‰é”®ã€‚

```html
<div class="ai-button-container">
    <a class="button ai-button" href="/ai-chat" rel="noopener" title="AIé—®ç­”">
        <span class="button-inner">
            ğŸ¤– AI é—®ç­”
        </span>
    </a>
</div>
```

åœ¨ CSS æ–‡ä»¶ä¸­å®šä¹‰è¿™ä¸ªæ–°çš„ ai-button-container å®¹å™¨ä»¥åŠæŒ‰é’®æœ¬èº«çš„æ ·å¼ã€‚

```css
.ai-button-container {
    width: 100%; /* ç¡®ä¿å®¹å™¨å æ®æ•´ä¸ªå®½åº¦ï¼Œä¸ºæŒ‰é’®æ¢è¡Œåšå‡†å¤‡ */
    margin-top: 0.5rem; /* ä¸ä¸Šä¸€è¡ŒæŒ‰é’®çš„é—´è· */
    display: flex;
    justify-content: center; /* è®©æŒ‰é’®åœ¨å®¹å™¨ä¸­æ°´å¹³å±…ä¸­ */
}

.button:active {
    transform: scale(0.96); /* ç‚¹å‡»æŒ‰é’®æ—¶ï¼Œä¼šæœ‰ä¸€ä¸ªç¼©å°çš„æ•ˆæœï¼Œ äº¤äº’ä¸Šå±•ç¤ºå‡ºç‚¹å‡»æœ‰æ•ˆ*/
}
```

ç„¶åå°±æ˜¯ AI é—®ç­”æœ¬èº«çš„é¡µé¢äº†ã€‚æ¯ä¸ªç½‘é¡µæœ€è¡¨å±‚æ˜¯ markdown æ–‡ä»¶ï¼Œæ”¾ç½®åœ¨ content æ–‡ä»¶å¤¹é‡Œï¼Œé‡Œé¢è¿˜æœ‰ about å…³äºã€æ—¶é—´è½´ã€å‹é“¾ã€æœç´¢çš„ md æ–‡ä»¶ã€‚æ–°å»ºä¸€ä¸ª ai-chat.md æ–‡ä»¶ï¼Œåœ¨é‡Œé¢å¼•ç”¨æ ·å¼ä¸º ai-chatã€‚

```markdown
---
title: "ğŸ¤– AIé—®ç­”"
layout: ai-chat
---
```

åœ¨ layouts/_default/ ç›®å½•ä¸‹ï¼Œæ–°å»º ai-chat.html æ–‡ä»¶ã€‚å¯¹è¯é¡µé¢çš„æ‰€æœ‰å‰ç«¯ HTML ä»£ç éƒ½åœ¨è¿™é‡Œé¢å®šä¹‰ã€‚

å¼€å¤´ä¸¤æ®µ \<head> å’Œ \<header> ä»£ç æ˜¯é¡µé¢çš„å¤´éƒ¨å’Œå…ƒä¿¡æ¯éƒ¨åˆ†ï¼Œå¯ä»¥ä»åŒä¸€ç›®å½•ä¸‹çš„å…¶ä»–æ–‡ä»¶é‡Œå¤åˆ¶è¿‡æ¥ã€‚

```html
<div id="searchbox">
    <h2>å‘æˆ‘æé—®å§</h2>
    <div class="input-with-button"> <!-- ç”¨æˆ·è¾“å…¥æ¡†çš„å®¹å™¨ -->
        <textarea id="question-input" placeholder="åœ¨è¿™é‡Œè¾“å…¥" aria-label="è¾“å…¥ä½ è¦é—®AIçš„é—®é¢˜" rows="1" oninput="autoResize(this)" onkeypress="if(event.key === 'Enter'){event.preventDefault(); askAI();}"></textarea> 
        <button onclick="askAI()" class="inline-button" aria-label="å‘é€é—®é¢˜">å‘é€</button>
    </div>
</div>
```

ç›¸åº”çš„ CSS æ ·å¼åœ¨ assets/css/extended/transition.css è¿™ä¸ªæ–‡ä»¶é‡Œå®šä¹‰ã€‚
```css
/* è¾“å…¥æ¡†å’ŒæŒ‰é’®çš„åŒ…è£…å®¹å™¨ */
.input-with-button {
    position: relative; /* ä¸ºç»å¯¹å®šä½çš„æŒ‰é’®æä¾›å‚è€ƒ */
    display: flex;
    width: 100%; /* ä¸æœç´¢æ¡†åŒå®½ */
    max-width: 800px; /* é™åˆ¶æœ€å¤§å®½åº¦ï¼Œä¸åŸæœ‰è®¾è®¡ä¿æŒä¸€è‡´ */
    margin: 0 auto; /* å±…ä¸­æ˜¾ç¤º */
    margin-top: 0.5rem;
}

/* è¾“å…¥æ¡†æ ·å¼ */
#question-input {
    flex: 1; /* å æ®å‰©ä½™æ‰€æœ‰ç©ºé—´ */
    padding-right: 70px; /* ä¸ºæŒ‰é’®é¢„ç•™ç©ºé—´ï¼Œé˜²æ­¢æ–‡å­—è¢«é®æŒ¡ */
    padding: 10px 70px 12px 15px;
    border: 2px solid;
    border-color: var(--tertiary);
    border-radius: 12px; /* åœ†è§’è¾¹æ¡† */
    font-size: 16px;
    box-sizing: border-box; /* ç¡®ä¿paddingä¸ä¼šå½±å“æ€»å®½åº¦ */
    resize: none;
    overflow-y: hidden;
    width: 100%;
    transition: height 0.2s ease;
}

/* å†…åµŒæœç´¢æŒ‰é’®æ ·å¼ */
.inline-button {
    position: absolute;
    right: 4px; /* è·ç¦»å³ä¾§4px */
    bottom: 4px; /* è·ç¦»é¡¶éƒ¨4px */
    height: 36px; /* æ¯”è¾“å…¥æ¡†ç¨çŸ® */
    padding: 0 16px;
    background: var(--primary); /* ä½¿ç”¨é¢œè‰²ä¸»é¢˜ï¼Œä¸»é¢˜æ–‡ä»¶åœ¨ themes/hugo-PaperMod/assets/css/core/theme-vars.css */
    border: none;
    border-radius: 20px; /* ç¨å°çš„åœ†è§’ */
    color: white;
    font-weight: 600;
    cursor: pointer;
    transition: background-color 0.2s ease;
}

/* æŒ‰é’®æ‚¬åœæ•ˆæœï¼Œä¼šæ”¾å¤§ä¸€ç‚¹ */
.inline-button:hover {
    background: var(--primary); /* ä½¿ç”¨ä¸»é¢˜çš„ä¸»è‰²å˜é‡ */
    transform: scale(1.02);
}

/*å½“ç‚¹å‡»å‘é€åï¼ŒæŒ‰é’®æš‚æ—¶ç¦ç”¨çš„æ•ˆæœ*/
.inline-button:disabled {
    opacity: 0.6;
    cursor: not-allowed;
    transform: none;
}
```

æœ€åˆç‰ˆæœ¬çš„å‘é€æ ·å¼å°±åªæœ‰è¿™äº›ï¼Œåªæœ‰æœ€åŸºæœ¬çš„è¾“å…¥æ–‡æœ¬æ¡†å’Œå‘é€é”®ã€‚åæ¥è§‰å¾—è¿™æ ·çœ‹èµ·æ¥å¤ªç©ºæ—·äº†ï¼Œäºæ˜¯åŠ äº†ä¸€ä¸ªæŒ¥æ‰‹çš„å°ç†Šå›¾ç‰‡ï¼Œä»¥åŠå°ç†Šè¯´çš„è¯ï¼Œæ¥å¼•å¯¼ç”¨æˆ·çš„è¾“å…¥ã€‚ç”¨æˆ·åœ¨ç‚¹å‡»å‘é€åï¼Œå°ç†Šä¼šé€€åœºï¼ŒçœŸå®å“åº”æ¡†ä¼šå–ä»£è¿™ä¸ªä½ç½®ã€‚è¿™ä¸¤æ®µéƒ½è¢«åŒ…è£…åœ¨ä¸€ä¸ª api-response-container å®¹å™¨é‡Œã€‚

```html
<div id="api-response-container" style="margin-top: 2rem;">
    <!-- å°ç†Šå ä½ç¬¦ -->
    <div id="placeholder" class="chat-placeholder">
        <!--<img src="Bear.jpg" alt="æœºå™¨ç†Š" class="bear-avatar"> -->
        <img src="{{ "/img/Bearwave.png" | relURL }}" alt="å°ç†Š" class="bear-avatar">

        <div class="speech-bubble">
            ä½ å¥½ï¼æˆ‘æ˜¯æœºå™¨ç†Šï¼Œæ˜¯ç†Šç†Šçš„åŠ©ç†ï¼Œæˆ‘å¯ä»¥å¸® TA å›ç­”ä½ çš„é—®é¢˜ã€‚ä½ å¯ä»¥é—®æˆ‘è·Ÿç®€å†æœ‰å…³çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥éšä¾¿èŠèŠã€‚å°½ç®¡æé—®å§ï¼
        </div>
    </div>

    <!-- çœŸå®å“åº”æ¡† -->
    <div id="real-response" class="response-container" style="display: none;">
        <div class="response-header">
            <h2>AIå›ç­”</h2>
        </div>
        <div id="api-response-content" class="response-content">
            <!-- APIè¿”å›çš„å†…å®¹å°†åœ¨è¿™é‡Œæ˜¾ç¤º -->
        </div>
    </div>
</div>
```

å¯¹åº”çš„ CSS é…ç½®å¦‚ä¸‹ã€‚

```css
/* å°ç†ŠèŠå¤©å ä½ç¬¦å¸ƒå±€ */
.chat-placeholder {
    display: flex;
    align-items: flex-start;
    gap: 12px;
    padding: 15px;
    opacity: 0; /* åˆå§‹éšè— */
    transition: opacity 0.8s ease-in-out; /* æ·¡å…¥è¿‡æ¸¡ */
}

/* ä¸ºäº†é˜²æ­¢åŠ è½½å¡é¡¿ï¼Œåœ¨å°ç†Šå›¾ç‰‡èµ„æºåŠ è½½å®Œæˆåå†æ·¡å…¥æ˜¾ç¤º */
.chat-placeholder.loaded {
    opacity: 1;
}

/* å°ç†Šå¤´åƒ */
img.bear-avatar {
    border: none !important;
    width: 150px;
    height: auto;
    flex-shrink: 0;
    box-shadow: none !important; /* å»æ‰é˜´å½± */
    padding: 0 !important;     /* å»æ‰å†…è¾¹è· */
    background: transparent !important; /* é¿å…èƒŒæ™¯è‰² */
}

/* æ°”æ³¡æ ·å¼ */
.speech-bubble {
    position: relative;
    background: #fff;
    border: 2px solid var(--tertiary);
    border-radius: 12px;
    padding: 12px 16px;
    font-size: 16px;
    line-height: 1.6;
    max-width: 60%;
    text-align: left;
}

/* æ°”æ³¡å°å°¾å·´ï¼ˆæŒ‡å‘å°ç†Šå¤´åƒï¼‰ */
.speech-bubble {
    content: "";
    position: absolute;
    top: 15px;
    left: -12px; /* è®©å°¾å·´è¿åˆ°å°ç†Š */
    border-width: 10px 12px 10px 0;
    border-style: solid;
    border-color: transparent #fff transparent transparent;
}

/* æ·¡å…¥çš„åŠ¨ç”»æ•ˆæœ */
@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}
```

åˆ°è¿™é‡Œä¸ºæ­¢ï¼Œçº¯è§†è§‰ä¸Šçš„æ•ˆæœå°±è®¾è®¡å®Œäº†ã€‚

## åŠ¨æ€æ•ˆæœå’Œå‰åç«¯äº¤äº’

æ¥ä¸‹æ¥éœ€è¦åœ¨ HTML æ–‡ä»¶é‡Œä½¿ç”¨ \<script> æ ‡ç­¾ï¼Œå®šä¹‰ä¸€äº›åŠ¨æ€æ•ˆæœï¼Œä»¥åŠå‰åç«¯çš„äº¤äº’é€»è¾‘ã€‚

åŠ¨æ€æ•ˆæœå¦‚ä¸‹ï¼š

```javascript
// ç”¨æˆ·è¾“å…¥æ¡†è‡ªåŠ¨æ ¹æ®è¡Œæ•°è°ƒæ•´é«˜åº¦
function autoResize(textarea) {
    console.log('autoResize function called');
    textarea.style.height = 'auto';
    textarea.style.height = textarea.scrollHeight + 'px';
    
    // é™åˆ¶æœ€å¤§é«˜åº¦
    if (textarea.scrollHeight > 200) {
        textarea.style.height = '200px';
        textarea.style.overflowY = 'auto';
    } else {
        textarea.style.overflowY = 'hidden';
    }
}

// é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–
document.addEventListener('DOMContentLoaded', function() {
    const textarea = document.getElementById('question-input');
    if (textarea) {
        // åˆå§‹è°ƒæ•´ä¸€æ¬¡é«˜åº¦
        setTimeout(() => autoResize(textarea), 100);
    }
});
```

ç„¶åæ¥åˆ°é‡å¤´æˆï¼Œå°±æ˜¯ askAI() å’Œ callAIApi() è¿™ä¸¤ä¸ªå‡½æ•°çš„é€»è¾‘ã€‚

askAI() å‡½æ•°å¤„ç†äº†å¤§éƒ¨åˆ†é€»è¾‘ï¼ŒåŒ…æ‹¬æ¥æ”¶ç”¨æˆ·çš„æé—®ã€åŠ è½½æ—¶çš„åŠ¨æ•ˆã€è°ƒç”¨ callAIApi å»è§¦å‘ HTTP å“åº”ã€å¤„ç†æµå¼ä¼ è¾“å“åº”çš„å±•ç¤ºã€‚ä¸€äº›æŒ‡æ ‡ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œè®°å½•æŠ“å–ï¼Œæ¯”å¦‚å»¶è¿Ÿã€‚æµå¼å“åº”çš„æ•ˆæœå°±æ˜¯ AI ä¸€è¾¹ç”Ÿæˆã€é¡µé¢ä¸€è¾¹é™†ç»­å±•ç¤ºå·²ç»ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œä¸å¿…ç­‰åˆ°å…¨éƒ¨ç”Ÿæˆå®Œå›ç­”å†ä¸€æ¬¡æ€§å±•ç¤ºï¼Œè¿™æ ·èƒ½å‡å°‘ç”¨æˆ·çš„ç­‰å¾…æ—¶é—´ã€‚ä½†æµå¼å“åº”çš„å¤„ç†é€»è¾‘ä¹Ÿæ¯”æ•´æ®µ response ç›´æ¥è¾“å‡ºè¦å¤æ‚å¾ˆå¤šï¼Œéœ€è¦æŒ‰è¡Œå»è§£ææ–‡æœ¬ï¼›å“åº”æ ¼å¼ä¹Ÿä» JSON å˜æˆäº† StreamingResponseã€‚

```javascript
async function askAI() {
    const startTime = performance.now();  // ç”¨æˆ·ç‚¹å‡»æ—¶é—´
    const question = document.getElementById('question-input').value.trim(); // è·å–ç”¨æˆ·çš„é—®é¢˜
    const responseContainer = document.getElementById('api-response-container');
    const responseContent = document.getElementById('api-response-content');
    const placeholder = document.getElementById('placeholder');   // å…ˆè·å–å°ç†Šï¼Œä¸ºäº†å“åº”å‡ºæ¥åéšè—å°ç†Š
    const realResponse = document.getElementById('real-response'); // è·å–å“åº”æ¡†
    responseContent.innerHTML = ""; // æ¸…ç©ºä¹‹å‰å†…å®¹
    
    if (!question) {
        alert('è¯·è¾“å…¥é—®é¢˜'); // å¦‚æœé—®é¢˜æ¡†ä¸ºç©ºå°±æŒ‰å‘é€ï¼Œæœ‰æŠ¥é”™æç¤º
        return;
    } 
    
    // ç‚¹å‡»å‘é€åï¼šéšè—å°ç†Šï¼Œæ˜¾ç¤ºå“åº”æ¡†
    placeholder.classList.add('hidden');
    realResponse.classList.remove('hidden');
    realResponse.classList.add('visible');
    
    // æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    responseContent.innerHTML = '<div class="loading">æ€è€ƒä¸­...</div>';
    responseContent.classList.add('loading');
    
    // ç¦ç”¨æŒ‰é’®é˜²æ­¢é‡å¤æäº¤
    const sendButton = document.querySelector('.inline-button');
    const originalText = sendButton.textContent;
    sendButton.textContent = 'æ€è€ƒä¸­...';
    sendButton.disabled = true;
    
    try {
        // è°ƒç”¨çœŸå®çš„API
        const response = await callAIApi(question); // callAIApi æ˜¯å‘é€ API è¯·æ±‚çš„å‡½æ•°ï¼Œåé¢æœ‰è¯¦ç»†å®šä¹‰
        const reader = response.body.getReader();
        const decoder = new TextDecoder("utf-8");

        // ç§»é™¤åŠ è½½çŠ¶æ€
        responseContent.classList.remove('loading');
        responseContent.innerHTML = "";

        // æ˜¾ç¤ºAPIè¿”å›çš„å†…å®¹ï¼Œç”¨æµå¼å“åº”çš„é€»è¾‘æ¥å¤„ç†
        let done = false;
        let buffer = "";
        let fullAnswer = "";
        while (true) {
            const { value, done } = await reader.read();
            if (done) break;
            const chunk = decoder.decode(value, { stream: true });
            buffer += chunk;

            // SSE æŒ‰è¡Œå¤„ç†
            let lines = buffer.split(/\r?\n/); // æŒ‰ç…§æ¢è¡Œç¬¦æ¥åˆ‡åˆ†å¥å­
            buffer = lines.pop(); // æœ€åä¸€è¡Œå¯èƒ½ä¸å®Œæ•´ï¼Œä¿ç•™åˆ°ä¸‹ä¸€è½®

            for (let line of lines) {
                line = line.trim();
                if (!line) continue;
                if (line === "data: [DONE]") continue;

                if (line.startsWith("data: ")) {
                    const jsonStr = line.slice(6);
                    try {
                        const parsed = JSON.parse(jsonStr);
                        const content = parsed.choices?.[0]?.delta?.content;
                        if (content) {
                            fullAnswer += content; // fullAnswer æ˜¯ç”¨äºåœ¨ Grafana é‡Œè®°å½•å›ç­”å†å²å’Œè®¡ç®—å»¶è¿Ÿ
                            // ç›´æ¥æ›´æ–°å†…å®¹å¹¶åº”ç”¨åŠ¨ç”»
                            updateContentWithAnimation(responseContent, fullAnswer);
                        }
                    } catch (e) {
                        // è¿™é‡Œåªæ˜¯æ—¥å¿—ï¼Œä¸‹ä¸€è½®ç»§ç»­æ‹¼æ¥
                        console.warn("è§£æ chunk é”™è¯¯:", e, jsonStr);
                    }
                }
            }
        }

        const endTime = performance.now();         // è®°å½•å“åº”è¿”å›æ—¶é—´
        const latencyMs = Math.round(endTime - startTime); // è®¡ç®—è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        console.log(`æœ¬æ¬¡è€—æ—¶: ${latencyMs} ms`);

        // å‘é€ç»™åç«¯å»¶æ—¶çš„æ•°æ®
        await fetch('https://api.bearlybear.com/api/record_latency', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({ question, answer: fullAnswer, latency_ms: latencyMs })
        });
        
    } catch (error) {
        responseContent.classList.remove('loading');
        responseContent.innerHTML = `
            <div class="error">
                <p>å‡ºé”™å•¦: ${error.message}</p>
                <p>è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥</p>
            </div>
        `;
    } finally {
        // æ¢å¤æŒ‰é’®çŠ¶æ€
        sendButton.textContent = originalText;
        sendButton.disabled = false;
    }
}
```

å…³äºæµå¼è¾“å‡ºï¼Œè¿™é‡Œå€¼å¾—å•ç‹¬å†™ä¸€ä¸‹ã€‚åœ¨å¤„ç†è¿™ä¸ªé€»è¾‘çš„æ—¶å€™é‡åˆ°äº†å¾ˆå¤šæŠ¥é”™ï¼Œè¿˜æ›¾ç»åœ¨é¡µé¢ä¸Šè¯¯å±•ç¤ºè¿‡å¾ˆå¤šå¸¦æ ‡ç­¾å¯¹çš„ token å‡ºæ¥ã€‚è¦æ³¨æ„ä¿®æ”¹å‡ ä¸ªåœ°æ–¹ï¼š

1. åç«¯çš„ app.py é‡Œè°ƒç”¨ API çš„æ¥å£ï¼Œéœ€è¦æŠŠ Stream å‚æ•°è®¾ç½®ä¸º Trueã€‚å¹¶ä¸”åœ¨ API çš„è·¯ç”±å‡½æ•°é‡Œï¼Œè¦æŠŠè¿”å›ç±»å‹è®¾ç½®ä¸º StreamingResponse è€ŒéåŸæ¥çš„ JSONã€‚
```python
def get_ai_response(prompt):
    # ...
    data = {
    "model": "deepseek-chat",  # æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©æ¨¡å‹
    "messages": [{"role": "user", "content": prompt}],
    "temperature": 0.7,
    "stream": True
    }

    try:
        with requests.post(url, json=data, headers=headers, stream=True, timeout=30) as r:
            r.raise_for_status()
            for line in r.iter_lines(decode_unicode=True):
                if line:
                    yield line + "\n"  # æ¯è¡Œé€å—è½¬å‘ç»™å‰ç«¯
    except requests.exceptions.RequestException as e:
        yield f"data: {json.dumps({'error': f'Deepseek API error: {str(e)}'})}\n\n"

@app.post("/api/ask_stream")
async def ask_stream(request: QuestionRequest):
    prompt = request.question
    return StreamingResponse(get_ai_response(prompt), media_type="text/event-stream")
```

2. å‰ç«¯å¯¹æµå¼è¾“å‡ºçš„å¤„ç†æ ¼å¼ã€‚è§ä¸Šæ–¹çš„ askAI å‡½æ•°å®šä¹‰ã€‚

é‡åˆ°çš„é—®é¢˜ä¸€ï¼šåœ¨è°ƒè¯•è¿‡ç¨‹ä¸­ï¼ŒæŸæ¬¡çœ‹åˆ°ç½‘é¡µè¿”å›äº†å¾ˆå¤šåƒè¿™æ ·çš„ç»“æœï¼Œå¸¦ç€ SSE æµå¼è¾“å‡ºçš„åŸå§‹æ ¼å¼å°±å±•ç¤ºåœ¨é¡µé¢ä¸Šï¼š

data: {"id":"3eaf3e26-564c-4f5b-a5bf-640f39ed8d87","object":"chat.completion.chunk","created":1759677785,"model":"deepseek-chat","system_fingerprint":"fp_ffc7281d48_prod0820_fp8_kvcache","choices":[{"index":0,"delta":{"content":"å¥½çš„"},"logprobs":null,"finish_reason":null}]}

å®é™…ä¸Šè¿™äº›æ˜¯ token çº§åˆ«çš„ JSONï¼Œè€Œä¸æ˜¯ç›´æ¥å¯æ˜¾ç¤ºçš„æ–‡æœ¬ã€‚æˆ‘åœ¨åç«¯æŠŠåŸæœ¬çš„ JSON çš„ text/plain æ•´æ®µè¿”å›æ ¼å¼æ”¹ä¸ºäº† text/event-stream ä¹‹åï¼Œæ¯æ¬¡å½“åç«¯ä»¥ SSEï¼ˆtext/event-streamï¼‰å‘é€æ•°æ®æ—¶ï¼ŒHTTP è¿æ¥ä¸ä¼šç«‹å³ç»“æŸï¼Œè€Œæ˜¯ä¸æ–­æ¨é€ä¸Šé¢è¿™æ ·ä»¥ data å¼€å¤´çš„æ•°æ®æµã€‚å¦‚æœæ­¤æ—¶ä¸é’ˆå¯¹è¿™æ ·çš„æ•°æ®æ ¼å¼åšå¤„ç†ï¼Œè€Œæ˜¯ç›´æ¥å¯¹æ•´æ®µå­—ç¬¦ä¸²ï¼ˆåŒ…å« data:ã€ç©ºè¡Œã€æ¢è¡Œç¬¦ï¼‰æ‰§è¡Œäº† JSON.parse()ï¼Œå°±ä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ã€‚æ‰€ä»¥å‰ç«¯çš„ askAI é‡Œé¢å¯¹æµå¼æ–‡æœ¬çš„è§£ææœ‰è¯¯ã€‚

è§£å†³æ–¹æ³•æ˜¯ï¼Œæ£€æµ‹ä»¥ data å¼€å¤´çš„æ•°æ®æµï¼ŒæŠŠå†—ä½™æ ‡ç­¾å‰¥é™¤ï¼Œç„¶åè¯»å–é‡Œé¢çš„ content ä¹Ÿå°±æ˜¯æœ‰æ•ˆå›ç­”çš„æ–‡æœ¬ã€‚

```javascript
// SSE æŒ‰è¡Œå¤„ç†
let lines = buffer.split(/\r?\n/); // æŒ‰ç…§æ¢è¡Œç¬¦æ¥åˆ‡åˆ†å¥å­
buffer = lines.pop(); // æœ€åä¸€è¡Œå¯èƒ½ä¸å®Œæ•´ï¼Œä¿ç•™åˆ°ä¸‹ä¸€è½®

for (let line of lines) {
    line = line.trim();
    if (!line) continue;
    if (line === "data: [DONE]") continue;

    if (line.startsWith("data: ")) {
        const jsonStr = line.slice(6); 
        try {
            const parsed = JSON.parse(jsonStr);
            const content = parsed.choices?.[0]?.delta?.content;
        ...}
```

é‡åˆ°çš„é—®é¢˜äºŒï¼šå‡ºç°äº†å›ç­”æ¼å­—çš„æƒ…å†µï¼Œå¯ä»¥æ˜æ˜¾çœ‹åˆ°è¯­è¨€ä¸è¿è´¯ï¼Œä¸€äº›å­—ç¼ºå¤±äº†ã€‚é¡µé¢ F12 æ§åˆ¶å°ä¸­æœ‰å¦‚ä¸‹æŠ¥é”™ï¼š

```console
è§£æ chunk é”™è¯¯: SyntaxError: Expected ':' after property name...è§£æ chunk é”™è¯¯: SyntaxError: Unterminated string in JSON at position 167 (line 1 column 168) at JSON.parse (<anonymous>) at askAI (ai-chat/:420:45) data: {"id":"04d88ed3-bae9-42eb-8cfd-4780be317940","object":"chat.completion.chunk","created":1759679644,"model":"deepseek-chat","system_fingerprint":"fp_ffc7281d48_prod0820 askAI @ ai-chat/:427 ai-chat/:427
```

è¿™æ˜¯å› ä¸º DeepSeek SSE æµæ˜¯é€è¡Œå‘é€ JSON å—ï¼Œä»¥æ¢è¡Œç¬¦ \n ä½œä¸ºç»“å°¾ã€‚ç„¶è€Œåœ¨ HTTP/1.1 çš„åˆ†å—ä¼ è¾“ç¼–ç ï¼ˆChunked Transfer Encodingï¼‰æœºåˆ¶ä¸­ï¼Œå½“æœåŠ¡å™¨ä»¥æµçš„æ–¹å¼è¿”å›å“åº”æ—¶ï¼ˆæ¯”å¦‚ Content-Type: text/event-streamï¼‰ï¼ŒHTTP ä¸å†æå‰å£°æ˜ Content-Lengthï¼Œè€Œæ˜¯æŠŠå“åº”æ‹†æˆä¸€æ®µä¸€æ®µçš„ chunk ä¼ è¾“ã€‚æ¯ä¸ª chunk çš„å¤§å°å’Œè¾¹ç•Œç”±åº•å±‚ TCP è¿æ¥å†³å®šï¼Œè€Œä¸æ˜¯åº”ç”¨å±‚ï¼ˆFastAPI æˆ–å‰ç«¯ï¼‰èƒ½æ§åˆ¶çš„ã€‚æ‰€ä»¥ï¼Œæ¯ä¸ª chunk çš„è¾¹ç•Œæ˜¯ä»»æ„çš„ï¼Œä¸€æ¡ data å¯èƒ½è¢«æˆªæ–­ï¼Œè€Œæ­¤æ—¶è¿˜æ²¡æœ‰åˆ°è¾¾ SSE åè®®ä¸­æ¯æ¡æ¶ˆæ¯çš„æ¢è¡Œç¬¦ \n ç»“å°¾ï¼Œå› æ­¤è¿™æ¡ data åé¢æœªå®Œçš„å†…å®¹å¯èƒ½å°±è¢«æŠ›å¼ƒäº†ï¼Œä¸‹ä¸€æ¬¡è¯»å–çš„æ—¶å€™åˆä»æœ€æ–°çš„ data å¼€å§‹ã€‚è€Œåœ¨æ¯æ¬¡ read è¯»åˆ°æ•°æ®å°±ç›´æ¥ JSON.parse è§£ææˆäº† lineï¼Œæ­¤æ—¶æ¯ä¸ª line æœ‰å¯èƒ½åªæ˜¯ä¸€éƒ¨åˆ† JSON å¯¹è±¡ï¼Œæ‰€ä»¥å¯¼è‡´æ¼æ‰äº†ä¸€äº›å­—ã€‚

ä¿®æ”¹æ€è·¯æ˜¯ï¼Œæ¯æ¬¡ reader.read() è¯»åˆ°ä¸€ä¸ª chunkï¼Œå…ˆä¸è¦å» parseï¼Œè€Œæ˜¯å…ˆæŠŠå®ƒ decode() æˆå­—ç¬¦ä¸²ã€‚ç„¶åï¼Œå¢åŠ ä¸€ä¸ª buffer ç¼“å†²åŒºå¤„ç†ï¼Œå¦‚æœ line åªæ˜¯ä¸€éƒ¨åˆ† JSONï¼ˆè¢«æˆªæ–­äº†ï¼‰ï¼Œå°±å…ˆä¿å­˜åœ¨ bufferï¼Œç­‰ä¸‹ä¸€æ¬¡ read chunk å†æ‹¼æ¥æˆå®Œæ•´ JSONã€‚é‡åˆ° data: [DONE] è¡¨ç¤ºæµç»“æŸã€‚

```javascript
let done = false;
let buffer = "";
let fullAnswer = "";
while (true) {
    const { value, done } = await reader.read(); // å¼‚æ­¥è¯»å–æµçš„ä¸€å°æ®µæ•°æ®
    if (done) break;
    const chunk = decoder.decode(value, { stream: true }); // æŠŠå­—èŠ‚æµè½¬ä¸º UTF-8 å­—ç¬¦ä¸²
    buffer += chunk; // æ‹¼æ¥å½“å‰æ•°æ®å—åˆ°ç¼“å­˜ã€‚

    // SSE æŒ‰è¡Œå¤„ç†
    let lines = buffer.split(/\r?\n/); // æŒ‰ç…§æ¢è¡Œç¬¦æ¥åˆ‡åˆ†å¥å­
    buffer = lines.pop(); // æœ€åä¸€è¡Œå¯èƒ½ä¸å®Œæ•´ï¼Œä¿ç•™åˆ°ä¸‹ä¸€è½®

    // å†æ¥é€è¡Œè§£æçš„é€»è¾‘
```

ï¼ˆæœªå®Œå¾…ç»­ï¼‰